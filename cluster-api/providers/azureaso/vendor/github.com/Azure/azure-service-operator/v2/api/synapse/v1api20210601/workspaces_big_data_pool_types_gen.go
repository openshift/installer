// Code generated by azure-service-operator-codegen. DO NOT EDIT.
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.
package v1api20210601

import (
	"fmt"
	arm "github.com/Azure/azure-service-operator/v2/api/synapse/v1api20210601/arm"
	storage "github.com/Azure/azure-service-operator/v2/api/synapse/v1api20210601/storage"
	"github.com/Azure/azure-service-operator/v2/internal/reflecthelpers"
	"github.com/Azure/azure-service-operator/v2/pkg/genruntime"
	"github.com/Azure/azure-service-operator/v2/pkg/genruntime/conditions"
	"github.com/Azure/azure-service-operator/v2/pkg/genruntime/configmaps"
	"github.com/Azure/azure-service-operator/v2/pkg/genruntime/core"
	"github.com/Azure/azure-service-operator/v2/pkg/genruntime/secrets"
	"github.com/pkg/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/conversion"
	"sigs.k8s.io/controller-runtime/pkg/webhook/admission"
)

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:name="Ready",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="Severity",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].severity"
// +kubebuilder:printcolumn:name="Reason",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].reason"
// +kubebuilder:printcolumn:name="Message",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].message"
// Generator information:
// - Generated from: /synapse/resource-manager/Microsoft.Synapse/stable/2021-06-01/bigDataPool.json
// - ARM URI: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Synapse/workspaces/{workspaceName}/bigDataPools/{bigDataPoolName}
type WorkspacesBigDataPool struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              WorkspacesBigDataPool_Spec   `json:"spec,omitempty"`
	Status            WorkspacesBigDataPool_STATUS `json:"status,omitempty"`
}

var _ conditions.Conditioner = &WorkspacesBigDataPool{}

// GetConditions returns the conditions of the resource
func (pool *WorkspacesBigDataPool) GetConditions() conditions.Conditions {
	return pool.Status.Conditions
}

// SetConditions sets the conditions on the resource status
func (pool *WorkspacesBigDataPool) SetConditions(conditions conditions.Conditions) {
	pool.Status.Conditions = conditions
}

var _ conversion.Convertible = &WorkspacesBigDataPool{}

// ConvertFrom populates our WorkspacesBigDataPool from the provided hub WorkspacesBigDataPool
func (pool *WorkspacesBigDataPool) ConvertFrom(hub conversion.Hub) error {
	source, ok := hub.(*storage.WorkspacesBigDataPool)
	if !ok {
		return fmt.Errorf("expected synapse/v1api20210601/storage/WorkspacesBigDataPool but received %T instead", hub)
	}

	return pool.AssignProperties_From_WorkspacesBigDataPool(source)
}

// ConvertTo populates the provided hub WorkspacesBigDataPool from our WorkspacesBigDataPool
func (pool *WorkspacesBigDataPool) ConvertTo(hub conversion.Hub) error {
	destination, ok := hub.(*storage.WorkspacesBigDataPool)
	if !ok {
		return fmt.Errorf("expected synapse/v1api20210601/storage/WorkspacesBigDataPool but received %T instead", hub)
	}

	return pool.AssignProperties_To_WorkspacesBigDataPool(destination)
}

// +kubebuilder:webhook:path=/mutate-synapse-azure-com-v1api20210601-workspacesbigdatapool,mutating=true,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=synapse.azure.com,resources=workspacesbigdatapools,verbs=create;update,versions=v1api20210601,name=default.v1api20210601.workspacesbigdatapools.synapse.azure.com,admissionReviewVersions=v1

var _ admission.Defaulter = &WorkspacesBigDataPool{}

// Default applies defaults to the WorkspacesBigDataPool resource
func (pool *WorkspacesBigDataPool) Default() {
	pool.defaultImpl()
	var temp any = pool
	if runtimeDefaulter, ok := temp.(genruntime.Defaulter); ok {
		runtimeDefaulter.CustomDefault()
	}
}

// defaultAzureName defaults the Azure name of the resource to the Kubernetes name
func (pool *WorkspacesBigDataPool) defaultAzureName() {
	if pool.Spec.AzureName == "" {
		pool.Spec.AzureName = pool.Name
	}
}

// defaultImpl applies the code generated defaults to the WorkspacesBigDataPool resource
func (pool *WorkspacesBigDataPool) defaultImpl() { pool.defaultAzureName() }

var _ configmaps.Exporter = &WorkspacesBigDataPool{}

// ConfigMapDestinationExpressions returns the Spec.OperatorSpec.ConfigMapExpressions property
func (pool *WorkspacesBigDataPool) ConfigMapDestinationExpressions() []*core.DestinationExpression {
	if pool.Spec.OperatorSpec == nil {
		return nil
	}
	return pool.Spec.OperatorSpec.ConfigMapExpressions
}

var _ secrets.Exporter = &WorkspacesBigDataPool{}

// SecretDestinationExpressions returns the Spec.OperatorSpec.SecretExpressions property
func (pool *WorkspacesBigDataPool) SecretDestinationExpressions() []*core.DestinationExpression {
	if pool.Spec.OperatorSpec == nil {
		return nil
	}
	return pool.Spec.OperatorSpec.SecretExpressions
}

var _ genruntime.ImportableResource = &WorkspacesBigDataPool{}

// InitializeSpec initializes the spec for this resource from the given status
func (pool *WorkspacesBigDataPool) InitializeSpec(status genruntime.ConvertibleStatus) error {
	if s, ok := status.(*WorkspacesBigDataPool_STATUS); ok {
		return pool.Spec.Initialize_From_WorkspacesBigDataPool_STATUS(s)
	}

	return fmt.Errorf("expected Status of type WorkspacesBigDataPool_STATUS but received %T instead", status)
}

var _ genruntime.KubernetesResource = &WorkspacesBigDataPool{}

// AzureName returns the Azure name of the resource
func (pool *WorkspacesBigDataPool) AzureName() string {
	return pool.Spec.AzureName
}

// GetAPIVersion returns the ARM API version of the resource. This is always "2021-06-01"
func (pool WorkspacesBigDataPool) GetAPIVersion() string {
	return "2021-06-01"
}

// GetResourceScope returns the scope of the resource
func (pool *WorkspacesBigDataPool) GetResourceScope() genruntime.ResourceScope {
	return genruntime.ResourceScopeResourceGroup
}

// GetSpec returns the specification of this resource
func (pool *WorkspacesBigDataPool) GetSpec() genruntime.ConvertibleSpec {
	return &pool.Spec
}

// GetStatus returns the status of this resource
func (pool *WorkspacesBigDataPool) GetStatus() genruntime.ConvertibleStatus {
	return &pool.Status
}

// GetSupportedOperations returns the operations supported by the resource
func (pool *WorkspacesBigDataPool) GetSupportedOperations() []genruntime.ResourceOperation {
	return []genruntime.ResourceOperation{
		genruntime.ResourceOperationDelete,
		genruntime.ResourceOperationGet,
		genruntime.ResourceOperationPut,
	}
}

// GetType returns the ARM Type of the resource. This is always "Microsoft.Synapse/workspaces/bigDataPools"
func (pool *WorkspacesBigDataPool) GetType() string {
	return "Microsoft.Synapse/workspaces/bigDataPools"
}

// NewEmptyStatus returns a new empty (blank) status
func (pool *WorkspacesBigDataPool) NewEmptyStatus() genruntime.ConvertibleStatus {
	return &WorkspacesBigDataPool_STATUS{}
}

// Owner returns the ResourceReference of the owner
func (pool *WorkspacesBigDataPool) Owner() *genruntime.ResourceReference {
	group, kind := genruntime.LookupOwnerGroupKind(pool.Spec)
	return pool.Spec.Owner.AsResourceReference(group, kind)
}

// SetStatus sets the status of this resource
func (pool *WorkspacesBigDataPool) SetStatus(status genruntime.ConvertibleStatus) error {
	// If we have exactly the right type of status, assign it
	if st, ok := status.(*WorkspacesBigDataPool_STATUS); ok {
		pool.Status = *st
		return nil
	}

	// Convert status to required version
	var st WorkspacesBigDataPool_STATUS
	err := status.ConvertStatusTo(&st)
	if err != nil {
		return errors.Wrap(err, "failed to convert status")
	}

	pool.Status = st
	return nil
}

// +kubebuilder:webhook:path=/validate-synapse-azure-com-v1api20210601-workspacesbigdatapool,mutating=false,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=synapse.azure.com,resources=workspacesbigdatapools,verbs=create;update,versions=v1api20210601,name=validate.v1api20210601.workspacesbigdatapools.synapse.azure.com,admissionReviewVersions=v1

var _ admission.Validator = &WorkspacesBigDataPool{}

// ValidateCreate validates the creation of the resource
func (pool *WorkspacesBigDataPool) ValidateCreate() (admission.Warnings, error) {
	validations := pool.createValidations()
	var temp any = pool
	if runtimeValidator, ok := temp.(genruntime.Validator); ok {
		validations = append(validations, runtimeValidator.CreateValidations()...)
	}
	return genruntime.ValidateCreate(validations)
}

// ValidateDelete validates the deletion of the resource
func (pool *WorkspacesBigDataPool) ValidateDelete() (admission.Warnings, error) {
	validations := pool.deleteValidations()
	var temp any = pool
	if runtimeValidator, ok := temp.(genruntime.Validator); ok {
		validations = append(validations, runtimeValidator.DeleteValidations()...)
	}
	return genruntime.ValidateDelete(validations)
}

// ValidateUpdate validates an update of the resource
func (pool *WorkspacesBigDataPool) ValidateUpdate(old runtime.Object) (admission.Warnings, error) {
	validations := pool.updateValidations()
	var temp any = pool
	if runtimeValidator, ok := temp.(genruntime.Validator); ok {
		validations = append(validations, runtimeValidator.UpdateValidations()...)
	}
	return genruntime.ValidateUpdate(old, validations)
}

// createValidations validates the creation of the resource
func (pool *WorkspacesBigDataPool) createValidations() []func() (admission.Warnings, error) {
	return []func() (admission.Warnings, error){pool.validateResourceReferences, pool.validateOwnerReference, pool.validateSecretDestinations, pool.validateConfigMapDestinations}
}

// deleteValidations validates the deletion of the resource
func (pool *WorkspacesBigDataPool) deleteValidations() []func() (admission.Warnings, error) {
	return nil
}

// updateValidations validates the update of the resource
func (pool *WorkspacesBigDataPool) updateValidations() []func(old runtime.Object) (admission.Warnings, error) {
	return []func(old runtime.Object) (admission.Warnings, error){
		func(old runtime.Object) (admission.Warnings, error) {
			return pool.validateResourceReferences()
		},
		pool.validateWriteOnceProperties,
		func(old runtime.Object) (admission.Warnings, error) {
			return pool.validateOwnerReference()
		},
		func(old runtime.Object) (admission.Warnings, error) {
			return pool.validateSecretDestinations()
		},
		func(old runtime.Object) (admission.Warnings, error) {
			return pool.validateConfigMapDestinations()
		},
	}
}

// validateConfigMapDestinations validates there are no colliding genruntime.ConfigMapDestinations
func (pool *WorkspacesBigDataPool) validateConfigMapDestinations() (admission.Warnings, error) {
	if pool.Spec.OperatorSpec == nil {
		return nil, nil
	}
	return configmaps.ValidateDestinations(pool, nil, pool.Spec.OperatorSpec.ConfigMapExpressions)
}

// validateOwnerReference validates the owner field
func (pool *WorkspacesBigDataPool) validateOwnerReference() (admission.Warnings, error) {
	return genruntime.ValidateOwner(pool)
}

// validateResourceReferences validates all resource references
func (pool *WorkspacesBigDataPool) validateResourceReferences() (admission.Warnings, error) {
	refs, err := reflecthelpers.FindResourceReferences(&pool.Spec)
	if err != nil {
		return nil, err
	}
	return genruntime.ValidateResourceReferences(refs)
}

// validateSecretDestinations validates there are no colliding genruntime.SecretDestination's
func (pool *WorkspacesBigDataPool) validateSecretDestinations() (admission.Warnings, error) {
	if pool.Spec.OperatorSpec == nil {
		return nil, nil
	}
	return secrets.ValidateDestinations(pool, nil, pool.Spec.OperatorSpec.SecretExpressions)
}

// validateWriteOnceProperties validates all WriteOnce properties
func (pool *WorkspacesBigDataPool) validateWriteOnceProperties(old runtime.Object) (admission.Warnings, error) {
	oldObj, ok := old.(*WorkspacesBigDataPool)
	if !ok {
		return nil, nil
	}

	return genruntime.ValidateWriteOnceProperties(oldObj, pool)
}

// AssignProperties_From_WorkspacesBigDataPool populates our WorkspacesBigDataPool from the provided source WorkspacesBigDataPool
func (pool *WorkspacesBigDataPool) AssignProperties_From_WorkspacesBigDataPool(source *storage.WorkspacesBigDataPool) error {

	// ObjectMeta
	pool.ObjectMeta = *source.ObjectMeta.DeepCopy()

	// Spec
	var spec WorkspacesBigDataPool_Spec
	err := spec.AssignProperties_From_WorkspacesBigDataPool_Spec(&source.Spec)
	if err != nil {
		return errors.Wrap(err, "calling AssignProperties_From_WorkspacesBigDataPool_Spec() to populate field Spec")
	}
	pool.Spec = spec

	// Status
	var status WorkspacesBigDataPool_STATUS
	err = status.AssignProperties_From_WorkspacesBigDataPool_STATUS(&source.Status)
	if err != nil {
		return errors.Wrap(err, "calling AssignProperties_From_WorkspacesBigDataPool_STATUS() to populate field Status")
	}
	pool.Status = status

	// No error
	return nil
}

// AssignProperties_To_WorkspacesBigDataPool populates the provided destination WorkspacesBigDataPool from our WorkspacesBigDataPool
func (pool *WorkspacesBigDataPool) AssignProperties_To_WorkspacesBigDataPool(destination *storage.WorkspacesBigDataPool) error {

	// ObjectMeta
	destination.ObjectMeta = *pool.ObjectMeta.DeepCopy()

	// Spec
	var spec storage.WorkspacesBigDataPool_Spec
	err := pool.Spec.AssignProperties_To_WorkspacesBigDataPool_Spec(&spec)
	if err != nil {
		return errors.Wrap(err, "calling AssignProperties_To_WorkspacesBigDataPool_Spec() to populate field Spec")
	}
	destination.Spec = spec

	// Status
	var status storage.WorkspacesBigDataPool_STATUS
	err = pool.Status.AssignProperties_To_WorkspacesBigDataPool_STATUS(&status)
	if err != nil {
		return errors.Wrap(err, "calling AssignProperties_To_WorkspacesBigDataPool_STATUS() to populate field Status")
	}
	destination.Status = status

	// No error
	return nil
}

// OriginalGVK returns a GroupValueKind for the original API version used to create the resource
func (pool *WorkspacesBigDataPool) OriginalGVK() *schema.GroupVersionKind {
	return &schema.GroupVersionKind{
		Group:   GroupVersion.Group,
		Version: pool.Spec.OriginalVersion(),
		Kind:    "WorkspacesBigDataPool",
	}
}

// +kubebuilder:object:root=true
// Generator information:
// - Generated from: /synapse/resource-manager/Microsoft.Synapse/stable/2021-06-01/bigDataPool.json
// - ARM URI: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Synapse/workspaces/{workspaceName}/bigDataPools/{bigDataPoolName}
type WorkspacesBigDataPoolList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []WorkspacesBigDataPool `json:"items"`
}

type WorkspacesBigDataPool_Spec struct {
	// AutoPause: Auto-pausing properties
	AutoPause *AutoPauseProperties `json:"autoPause,omitempty"`

	// AutoScale: Auto-scaling properties
	AutoScale *AutoScaleProperties `json:"autoScale,omitempty"`

	// AzureName: The name of the resource in Azure. This is often the same as the name of the resource in Kubernetes but it
	// doesn't have to be.
	AzureName string `json:"azureName,omitempty"`

	// CacheSize: The cache size
	CacheSize *int `json:"cacheSize,omitempty"`

	// CustomLibraries: List of custom libraries/packages associated with the spark pool.
	CustomLibraries []LibraryInfo `json:"customLibraries,omitempty"`

	// DefaultSparkLogFolder: The default folder where Spark logs will be written.
	DefaultSparkLogFolder *string `json:"defaultSparkLogFolder,omitempty"`

	// DynamicExecutorAllocation: Dynamic Executor Allocation
	DynamicExecutorAllocation *DynamicExecutorAllocation `json:"dynamicExecutorAllocation,omitempty"`

	// IsAutotuneEnabled: Whether autotune is required or not.
	IsAutotuneEnabled *bool `json:"isAutotuneEnabled,omitempty"`

	// IsComputeIsolationEnabled: Whether compute isolation is required or not.
	IsComputeIsolationEnabled *bool `json:"isComputeIsolationEnabled,omitempty"`

	// LibraryRequirements: Library version requirements
	LibraryRequirements *LibraryRequirements `json:"libraryRequirements,omitempty"`

	// +kubebuilder:validation:Required
	// Location: The geo-location where the resource lives
	Location *string `json:"location,omitempty"`

	// NodeCount: The number of nodes in the Big Data pool.
	NodeCount *int `json:"nodeCount,omitempty"`

	// NodeSize: The level of compute power that each node in the Big Data pool has.
	NodeSize *BigDataPoolResourceProperties_NodeSize `json:"nodeSize,omitempty"`

	// NodeSizeFamily: The kind of nodes that the Big Data pool provides.
	NodeSizeFamily *BigDataPoolResourceProperties_NodeSizeFamily `json:"nodeSizeFamily,omitempty"`

	// OperatorSpec: The specification for configuring operator behavior. This field is interpreted by the operator and not
	// passed directly to Azure
	OperatorSpec *WorkspacesBigDataPoolOperatorSpec `json:"operatorSpec,omitempty"`

	// +kubebuilder:validation:Required
	// Owner: The owner of the resource. The owner controls where the resource goes when it is deployed. The owner also
	// controls the resources lifecycle. When the owner is deleted the resource will also be deleted. Owner is expected to be a
	// reference to a synapse.azure.com/Workspace resource
	Owner *genruntime.KnownResourceReference `group:"synapse.azure.com" json:"owner,omitempty" kind:"Workspace"`

	// ProvisioningState: The state of the Big Data pool.
	ProvisioningState *string `json:"provisioningState,omitempty"`

	// SessionLevelPackagesEnabled: Whether session level packages enabled.
	SessionLevelPackagesEnabled *bool `json:"sessionLevelPackagesEnabled,omitempty"`

	// SparkConfigProperties: Spark configuration file to specify additional properties
	SparkConfigProperties *SparkConfigProperties `json:"sparkConfigProperties,omitempty"`

	// SparkEventsFolder: The Spark events folder
	SparkEventsFolder *string `json:"sparkEventsFolder,omitempty"`

	// SparkVersion: The Apache Spark version.
	SparkVersion *string `json:"sparkVersion,omitempty"`

	// Tags: Resource tags.
	Tags map[string]string `json:"tags,omitempty"`
}

var _ genruntime.ARMTransformer = &WorkspacesBigDataPool_Spec{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (pool *WorkspacesBigDataPool_Spec) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if pool == nil {
		return nil, nil
	}
	result := &arm.WorkspacesBigDataPool_Spec{}

	// Set property "Location":
	if pool.Location != nil {
		location := *pool.Location
		result.Location = &location
	}

	// Set property "Name":
	result.Name = resolved.Name

	// Set property "Properties":
	if pool.AutoPause != nil ||
		pool.AutoScale != nil ||
		pool.CacheSize != nil ||
		pool.CustomLibraries != nil ||
		pool.DefaultSparkLogFolder != nil ||
		pool.DynamicExecutorAllocation != nil ||
		pool.IsAutotuneEnabled != nil ||
		pool.IsComputeIsolationEnabled != nil ||
		pool.LibraryRequirements != nil ||
		pool.NodeCount != nil ||
		pool.NodeSize != nil ||
		pool.NodeSizeFamily != nil ||
		pool.ProvisioningState != nil ||
		pool.SessionLevelPackagesEnabled != nil ||
		pool.SparkConfigProperties != nil ||
		pool.SparkEventsFolder != nil ||
		pool.SparkVersion != nil {
		result.Properties = &arm.BigDataPoolResourceProperties{}
	}
	if pool.AutoPause != nil {
		autoPause_ARM, err := (*pool.AutoPause).ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		autoPause := *autoPause_ARM.(*arm.AutoPauseProperties)
		result.Properties.AutoPause = &autoPause
	}
	if pool.AutoScale != nil {
		autoScale_ARM, err := (*pool.AutoScale).ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		autoScale := *autoScale_ARM.(*arm.AutoScaleProperties)
		result.Properties.AutoScale = &autoScale
	}
	if pool.CacheSize != nil {
		cacheSize := *pool.CacheSize
		result.Properties.CacheSize = &cacheSize
	}
	for _, item := range pool.CustomLibraries {
		item_ARM, err := item.ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		result.Properties.CustomLibraries = append(result.Properties.CustomLibraries, *item_ARM.(*arm.LibraryInfo))
	}
	if pool.DefaultSparkLogFolder != nil {
		defaultSparkLogFolder := *pool.DefaultSparkLogFolder
		result.Properties.DefaultSparkLogFolder = &defaultSparkLogFolder
	}
	if pool.DynamicExecutorAllocation != nil {
		dynamicExecutorAllocation_ARM, err := (*pool.DynamicExecutorAllocation).ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		dynamicExecutorAllocation := *dynamicExecutorAllocation_ARM.(*arm.DynamicExecutorAllocation)
		result.Properties.DynamicExecutorAllocation = &dynamicExecutorAllocation
	}
	if pool.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *pool.IsAutotuneEnabled
		result.Properties.IsAutotuneEnabled = &isAutotuneEnabled
	}
	if pool.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *pool.IsComputeIsolationEnabled
		result.Properties.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	}
	if pool.LibraryRequirements != nil {
		libraryRequirements_ARM, err := (*pool.LibraryRequirements).ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		libraryRequirements := *libraryRequirements_ARM.(*arm.LibraryRequirements)
		result.Properties.LibraryRequirements = &libraryRequirements
	}
	if pool.NodeCount != nil {
		nodeCount := *pool.NodeCount
		result.Properties.NodeCount = &nodeCount
	}
	if pool.NodeSize != nil {
		var temp string
		temp = string(*pool.NodeSize)
		nodeSize := arm.BigDataPoolResourceProperties_NodeSize(temp)
		result.Properties.NodeSize = &nodeSize
	}
	if pool.NodeSizeFamily != nil {
		var temp string
		temp = string(*pool.NodeSizeFamily)
		nodeSizeFamily := arm.BigDataPoolResourceProperties_NodeSizeFamily(temp)
		result.Properties.NodeSizeFamily = &nodeSizeFamily
	}
	if pool.ProvisioningState != nil {
		provisioningState := *pool.ProvisioningState
		result.Properties.ProvisioningState = &provisioningState
	}
	if pool.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *pool.SessionLevelPackagesEnabled
		result.Properties.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	}
	if pool.SparkConfigProperties != nil {
		sparkConfigProperties_ARM, err := (*pool.SparkConfigProperties).ConvertToARM(resolved)
		if err != nil {
			return nil, err
		}
		sparkConfigProperties := *sparkConfigProperties_ARM.(*arm.SparkConfigProperties)
		result.Properties.SparkConfigProperties = &sparkConfigProperties
	}
	if pool.SparkEventsFolder != nil {
		sparkEventsFolder := *pool.SparkEventsFolder
		result.Properties.SparkEventsFolder = &sparkEventsFolder
	}
	if pool.SparkVersion != nil {
		sparkVersion := *pool.SparkVersion
		result.Properties.SparkVersion = &sparkVersion
	}

	// Set property "Tags":
	if pool.Tags != nil {
		result.Tags = make(map[string]string, len(pool.Tags))
		for key, value := range pool.Tags {
			result.Tags[key] = value
		}
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (pool *WorkspacesBigDataPool_Spec) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.WorkspacesBigDataPool_Spec{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (pool *WorkspacesBigDataPool_Spec) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.WorkspacesBigDataPool_Spec)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.WorkspacesBigDataPool_Spec, got %T", armInput)
	}

	// Set property "AutoPause":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.AutoPause != nil {
			var autoPause1 AutoPauseProperties
			err := autoPause1.PopulateFromARM(owner, *typedInput.Properties.AutoPause)
			if err != nil {
				return err
			}
			autoPause := autoPause1
			pool.AutoPause = &autoPause
		}
	}

	// Set property "AutoScale":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.AutoScale != nil {
			var autoScale1 AutoScaleProperties
			err := autoScale1.PopulateFromARM(owner, *typedInput.Properties.AutoScale)
			if err != nil {
				return err
			}
			autoScale := autoScale1
			pool.AutoScale = &autoScale
		}
	}

	// Set property "AzureName":
	pool.SetAzureName(genruntime.ExtractKubernetesResourceNameFromARMName(typedInput.Name))

	// Set property "CacheSize":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.CacheSize != nil {
			cacheSize := *typedInput.Properties.CacheSize
			pool.CacheSize = &cacheSize
		}
	}

	// Set property "CustomLibraries":
	// copying flattened property:
	if typedInput.Properties != nil {
		for _, item := range typedInput.Properties.CustomLibraries {
			var item1 LibraryInfo
			err := item1.PopulateFromARM(owner, item)
			if err != nil {
				return err
			}
			pool.CustomLibraries = append(pool.CustomLibraries, item1)
		}
	}

	// Set property "DefaultSparkLogFolder":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.DefaultSparkLogFolder != nil {
			defaultSparkLogFolder := *typedInput.Properties.DefaultSparkLogFolder
			pool.DefaultSparkLogFolder = &defaultSparkLogFolder
		}
	}

	// Set property "DynamicExecutorAllocation":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.DynamicExecutorAllocation != nil {
			var dynamicExecutorAllocation1 DynamicExecutorAllocation
			err := dynamicExecutorAllocation1.PopulateFromARM(owner, *typedInput.Properties.DynamicExecutorAllocation)
			if err != nil {
				return err
			}
			dynamicExecutorAllocation := dynamicExecutorAllocation1
			pool.DynamicExecutorAllocation = &dynamicExecutorAllocation
		}
	}

	// Set property "IsAutotuneEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.IsAutotuneEnabled != nil {
			isAutotuneEnabled := *typedInput.Properties.IsAutotuneEnabled
			pool.IsAutotuneEnabled = &isAutotuneEnabled
		}
	}

	// Set property "IsComputeIsolationEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.IsComputeIsolationEnabled != nil {
			isComputeIsolationEnabled := *typedInput.Properties.IsComputeIsolationEnabled
			pool.IsComputeIsolationEnabled = &isComputeIsolationEnabled
		}
	}

	// Set property "LibraryRequirements":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.LibraryRequirements != nil {
			var libraryRequirements1 LibraryRequirements
			err := libraryRequirements1.PopulateFromARM(owner, *typedInput.Properties.LibraryRequirements)
			if err != nil {
				return err
			}
			libraryRequirements := libraryRequirements1
			pool.LibraryRequirements = &libraryRequirements
		}
	}

	// Set property "Location":
	if typedInput.Location != nil {
		location := *typedInput.Location
		pool.Location = &location
	}

	// Set property "NodeCount":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeCount != nil {
			nodeCount := *typedInput.Properties.NodeCount
			pool.NodeCount = &nodeCount
		}
	}

	// Set property "NodeSize":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeSize != nil {
			var temp string
			temp = string(*typedInput.Properties.NodeSize)
			nodeSize := BigDataPoolResourceProperties_NodeSize(temp)
			pool.NodeSize = &nodeSize
		}
	}

	// Set property "NodeSizeFamily":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeSizeFamily != nil {
			var temp string
			temp = string(*typedInput.Properties.NodeSizeFamily)
			nodeSizeFamily := BigDataPoolResourceProperties_NodeSizeFamily(temp)
			pool.NodeSizeFamily = &nodeSizeFamily
		}
	}

	// no assignment for property "OperatorSpec"

	// Set property "Owner":
	pool.Owner = &genruntime.KnownResourceReference{
		Name:  owner.Name,
		ARMID: owner.ARMID,
	}

	// Set property "ProvisioningState":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.ProvisioningState != nil {
			provisioningState := *typedInput.Properties.ProvisioningState
			pool.ProvisioningState = &provisioningState
		}
	}

	// Set property "SessionLevelPackagesEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SessionLevelPackagesEnabled != nil {
			sessionLevelPackagesEnabled := *typedInput.Properties.SessionLevelPackagesEnabled
			pool.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
		}
	}

	// Set property "SparkConfigProperties":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkConfigProperties != nil {
			var sparkConfigProperties1 SparkConfigProperties
			err := sparkConfigProperties1.PopulateFromARM(owner, *typedInput.Properties.SparkConfigProperties)
			if err != nil {
				return err
			}
			sparkConfigProperties := sparkConfigProperties1
			pool.SparkConfigProperties = &sparkConfigProperties
		}
	}

	// Set property "SparkEventsFolder":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkEventsFolder != nil {
			sparkEventsFolder := *typedInput.Properties.SparkEventsFolder
			pool.SparkEventsFolder = &sparkEventsFolder
		}
	}

	// Set property "SparkVersion":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkVersion != nil {
			sparkVersion := *typedInput.Properties.SparkVersion
			pool.SparkVersion = &sparkVersion
		}
	}

	// Set property "Tags":
	if typedInput.Tags != nil {
		pool.Tags = make(map[string]string, len(typedInput.Tags))
		for key, value := range typedInput.Tags {
			pool.Tags[key] = value
		}
	}

	// No error
	return nil
}

var _ genruntime.ConvertibleSpec = &WorkspacesBigDataPool_Spec{}

// ConvertSpecFrom populates our WorkspacesBigDataPool_Spec from the provided source
func (pool *WorkspacesBigDataPool_Spec) ConvertSpecFrom(source genruntime.ConvertibleSpec) error {
	src, ok := source.(*storage.WorkspacesBigDataPool_Spec)
	if ok {
		// Populate our instance from source
		return pool.AssignProperties_From_WorkspacesBigDataPool_Spec(src)
	}

	// Convert to an intermediate form
	src = &storage.WorkspacesBigDataPool_Spec{}
	err := src.ConvertSpecFrom(source)
	if err != nil {
		return errors.Wrap(err, "initial step of conversion in ConvertSpecFrom()")
	}

	// Update our instance from src
	err = pool.AssignProperties_From_WorkspacesBigDataPool_Spec(src)
	if err != nil {
		return errors.Wrap(err, "final step of conversion in ConvertSpecFrom()")
	}

	return nil
}

// ConvertSpecTo populates the provided destination from our WorkspacesBigDataPool_Spec
func (pool *WorkspacesBigDataPool_Spec) ConvertSpecTo(destination genruntime.ConvertibleSpec) error {
	dst, ok := destination.(*storage.WorkspacesBigDataPool_Spec)
	if ok {
		// Populate destination from our instance
		return pool.AssignProperties_To_WorkspacesBigDataPool_Spec(dst)
	}

	// Convert to an intermediate form
	dst = &storage.WorkspacesBigDataPool_Spec{}
	err := pool.AssignProperties_To_WorkspacesBigDataPool_Spec(dst)
	if err != nil {
		return errors.Wrap(err, "initial step of conversion in ConvertSpecTo()")
	}

	// Update dst from our instance
	err = dst.ConvertSpecTo(destination)
	if err != nil {
		return errors.Wrap(err, "final step of conversion in ConvertSpecTo()")
	}

	return nil
}

// AssignProperties_From_WorkspacesBigDataPool_Spec populates our WorkspacesBigDataPool_Spec from the provided source WorkspacesBigDataPool_Spec
func (pool *WorkspacesBigDataPool_Spec) AssignProperties_From_WorkspacesBigDataPool_Spec(source *storage.WorkspacesBigDataPool_Spec) error {

	// AutoPause
	if source.AutoPause != nil {
		var autoPause AutoPauseProperties
		err := autoPause.AssignProperties_From_AutoPauseProperties(source.AutoPause)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_AutoPauseProperties() to populate field AutoPause")
		}
		pool.AutoPause = &autoPause
	} else {
		pool.AutoPause = nil
	}

	// AutoScale
	if source.AutoScale != nil {
		var autoScale AutoScaleProperties
		err := autoScale.AssignProperties_From_AutoScaleProperties(source.AutoScale)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_AutoScaleProperties() to populate field AutoScale")
		}
		pool.AutoScale = &autoScale
	} else {
		pool.AutoScale = nil
	}

	// AzureName
	pool.AzureName = source.AzureName

	// CacheSize
	pool.CacheSize = genruntime.ClonePointerToInt(source.CacheSize)

	// CustomLibraries
	if source.CustomLibraries != nil {
		customLibraryList := make([]LibraryInfo, len(source.CustomLibraries))
		for customLibraryIndex, customLibraryItem := range source.CustomLibraries {
			// Shadow the loop variable to avoid aliasing
			customLibraryItem := customLibraryItem
			var customLibrary LibraryInfo
			err := customLibrary.AssignProperties_From_LibraryInfo(&customLibraryItem)
			if err != nil {
				return errors.Wrap(err, "calling AssignProperties_From_LibraryInfo() to populate field CustomLibraries")
			}
			customLibraryList[customLibraryIndex] = customLibrary
		}
		pool.CustomLibraries = customLibraryList
	} else {
		pool.CustomLibraries = nil
	}

	// DefaultSparkLogFolder
	pool.DefaultSparkLogFolder = genruntime.ClonePointerToString(source.DefaultSparkLogFolder)

	// DynamicExecutorAllocation
	if source.DynamicExecutorAllocation != nil {
		var dynamicExecutorAllocation DynamicExecutorAllocation
		err := dynamicExecutorAllocation.AssignProperties_From_DynamicExecutorAllocation(source.DynamicExecutorAllocation)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_DynamicExecutorAllocation() to populate field DynamicExecutorAllocation")
		}
		pool.DynamicExecutorAllocation = &dynamicExecutorAllocation
	} else {
		pool.DynamicExecutorAllocation = nil
	}

	// IsAutotuneEnabled
	if source.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *source.IsAutotuneEnabled
		pool.IsAutotuneEnabled = &isAutotuneEnabled
	} else {
		pool.IsAutotuneEnabled = nil
	}

	// IsComputeIsolationEnabled
	if source.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *source.IsComputeIsolationEnabled
		pool.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	} else {
		pool.IsComputeIsolationEnabled = nil
	}

	// LibraryRequirements
	if source.LibraryRequirements != nil {
		var libraryRequirement LibraryRequirements
		err := libraryRequirement.AssignProperties_From_LibraryRequirements(source.LibraryRequirements)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_LibraryRequirements() to populate field LibraryRequirements")
		}
		pool.LibraryRequirements = &libraryRequirement
	} else {
		pool.LibraryRequirements = nil
	}

	// Location
	pool.Location = genruntime.ClonePointerToString(source.Location)

	// NodeCount
	pool.NodeCount = genruntime.ClonePointerToInt(source.NodeCount)

	// NodeSize
	if source.NodeSize != nil {
		nodeSize := *source.NodeSize
		nodeSizeTemp := genruntime.ToEnum(nodeSize, bigDataPoolResourceProperties_NodeSize_Values)
		pool.NodeSize = &nodeSizeTemp
	} else {
		pool.NodeSize = nil
	}

	// NodeSizeFamily
	if source.NodeSizeFamily != nil {
		nodeSizeFamily := *source.NodeSizeFamily
		nodeSizeFamilyTemp := genruntime.ToEnum(nodeSizeFamily, bigDataPoolResourceProperties_NodeSizeFamily_Values)
		pool.NodeSizeFamily = &nodeSizeFamilyTemp
	} else {
		pool.NodeSizeFamily = nil
	}

	// OperatorSpec
	if source.OperatorSpec != nil {
		var operatorSpec WorkspacesBigDataPoolOperatorSpec
		err := operatorSpec.AssignProperties_From_WorkspacesBigDataPoolOperatorSpec(source.OperatorSpec)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_WorkspacesBigDataPoolOperatorSpec() to populate field OperatorSpec")
		}
		pool.OperatorSpec = &operatorSpec
	} else {
		pool.OperatorSpec = nil
	}

	// Owner
	if source.Owner != nil {
		owner := source.Owner.Copy()
		pool.Owner = &owner
	} else {
		pool.Owner = nil
	}

	// ProvisioningState
	pool.ProvisioningState = genruntime.ClonePointerToString(source.ProvisioningState)

	// SessionLevelPackagesEnabled
	if source.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *source.SessionLevelPackagesEnabled
		pool.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	} else {
		pool.SessionLevelPackagesEnabled = nil
	}

	// SparkConfigProperties
	if source.SparkConfigProperties != nil {
		var sparkConfigProperty SparkConfigProperties
		err := sparkConfigProperty.AssignProperties_From_SparkConfigProperties(source.SparkConfigProperties)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_SparkConfigProperties() to populate field SparkConfigProperties")
		}
		pool.SparkConfigProperties = &sparkConfigProperty
	} else {
		pool.SparkConfigProperties = nil
	}

	// SparkEventsFolder
	pool.SparkEventsFolder = genruntime.ClonePointerToString(source.SparkEventsFolder)

	// SparkVersion
	pool.SparkVersion = genruntime.ClonePointerToString(source.SparkVersion)

	// Tags
	pool.Tags = genruntime.CloneMapOfStringToString(source.Tags)

	// No error
	return nil
}

// AssignProperties_To_WorkspacesBigDataPool_Spec populates the provided destination WorkspacesBigDataPool_Spec from our WorkspacesBigDataPool_Spec
func (pool *WorkspacesBigDataPool_Spec) AssignProperties_To_WorkspacesBigDataPool_Spec(destination *storage.WorkspacesBigDataPool_Spec) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// AutoPause
	if pool.AutoPause != nil {
		var autoPause storage.AutoPauseProperties
		err := pool.AutoPause.AssignProperties_To_AutoPauseProperties(&autoPause)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_AutoPauseProperties() to populate field AutoPause")
		}
		destination.AutoPause = &autoPause
	} else {
		destination.AutoPause = nil
	}

	// AutoScale
	if pool.AutoScale != nil {
		var autoScale storage.AutoScaleProperties
		err := pool.AutoScale.AssignProperties_To_AutoScaleProperties(&autoScale)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_AutoScaleProperties() to populate field AutoScale")
		}
		destination.AutoScale = &autoScale
	} else {
		destination.AutoScale = nil
	}

	// AzureName
	destination.AzureName = pool.AzureName

	// CacheSize
	destination.CacheSize = genruntime.ClonePointerToInt(pool.CacheSize)

	// CustomLibraries
	if pool.CustomLibraries != nil {
		customLibraryList := make([]storage.LibraryInfo, len(pool.CustomLibraries))
		for customLibraryIndex, customLibraryItem := range pool.CustomLibraries {
			// Shadow the loop variable to avoid aliasing
			customLibraryItem := customLibraryItem
			var customLibrary storage.LibraryInfo
			err := customLibraryItem.AssignProperties_To_LibraryInfo(&customLibrary)
			if err != nil {
				return errors.Wrap(err, "calling AssignProperties_To_LibraryInfo() to populate field CustomLibraries")
			}
			customLibraryList[customLibraryIndex] = customLibrary
		}
		destination.CustomLibraries = customLibraryList
	} else {
		destination.CustomLibraries = nil
	}

	// DefaultSparkLogFolder
	destination.DefaultSparkLogFolder = genruntime.ClonePointerToString(pool.DefaultSparkLogFolder)

	// DynamicExecutorAllocation
	if pool.DynamicExecutorAllocation != nil {
		var dynamicExecutorAllocation storage.DynamicExecutorAllocation
		err := pool.DynamicExecutorAllocation.AssignProperties_To_DynamicExecutorAllocation(&dynamicExecutorAllocation)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_DynamicExecutorAllocation() to populate field DynamicExecutorAllocation")
		}
		destination.DynamicExecutorAllocation = &dynamicExecutorAllocation
	} else {
		destination.DynamicExecutorAllocation = nil
	}

	// IsAutotuneEnabled
	if pool.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *pool.IsAutotuneEnabled
		destination.IsAutotuneEnabled = &isAutotuneEnabled
	} else {
		destination.IsAutotuneEnabled = nil
	}

	// IsComputeIsolationEnabled
	if pool.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *pool.IsComputeIsolationEnabled
		destination.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	} else {
		destination.IsComputeIsolationEnabled = nil
	}

	// LibraryRequirements
	if pool.LibraryRequirements != nil {
		var libraryRequirement storage.LibraryRequirements
		err := pool.LibraryRequirements.AssignProperties_To_LibraryRequirements(&libraryRequirement)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_LibraryRequirements() to populate field LibraryRequirements")
		}
		destination.LibraryRequirements = &libraryRequirement
	} else {
		destination.LibraryRequirements = nil
	}

	// Location
	destination.Location = genruntime.ClonePointerToString(pool.Location)

	// NodeCount
	destination.NodeCount = genruntime.ClonePointerToInt(pool.NodeCount)

	// NodeSize
	if pool.NodeSize != nil {
		nodeSize := string(*pool.NodeSize)
		destination.NodeSize = &nodeSize
	} else {
		destination.NodeSize = nil
	}

	// NodeSizeFamily
	if pool.NodeSizeFamily != nil {
		nodeSizeFamily := string(*pool.NodeSizeFamily)
		destination.NodeSizeFamily = &nodeSizeFamily
	} else {
		destination.NodeSizeFamily = nil
	}

	// OperatorSpec
	if pool.OperatorSpec != nil {
		var operatorSpec storage.WorkspacesBigDataPoolOperatorSpec
		err := pool.OperatorSpec.AssignProperties_To_WorkspacesBigDataPoolOperatorSpec(&operatorSpec)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_WorkspacesBigDataPoolOperatorSpec() to populate field OperatorSpec")
		}
		destination.OperatorSpec = &operatorSpec
	} else {
		destination.OperatorSpec = nil
	}

	// OriginalVersion
	destination.OriginalVersion = pool.OriginalVersion()

	// Owner
	if pool.Owner != nil {
		owner := pool.Owner.Copy()
		destination.Owner = &owner
	} else {
		destination.Owner = nil
	}

	// ProvisioningState
	destination.ProvisioningState = genruntime.ClonePointerToString(pool.ProvisioningState)

	// SessionLevelPackagesEnabled
	if pool.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *pool.SessionLevelPackagesEnabled
		destination.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	} else {
		destination.SessionLevelPackagesEnabled = nil
	}

	// SparkConfigProperties
	if pool.SparkConfigProperties != nil {
		var sparkConfigProperty storage.SparkConfigProperties
		err := pool.SparkConfigProperties.AssignProperties_To_SparkConfigProperties(&sparkConfigProperty)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_SparkConfigProperties() to populate field SparkConfigProperties")
		}
		destination.SparkConfigProperties = &sparkConfigProperty
	} else {
		destination.SparkConfigProperties = nil
	}

	// SparkEventsFolder
	destination.SparkEventsFolder = genruntime.ClonePointerToString(pool.SparkEventsFolder)

	// SparkVersion
	destination.SparkVersion = genruntime.ClonePointerToString(pool.SparkVersion)

	// Tags
	destination.Tags = genruntime.CloneMapOfStringToString(pool.Tags)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_WorkspacesBigDataPool_STATUS populates our WorkspacesBigDataPool_Spec from the provided source WorkspacesBigDataPool_STATUS
func (pool *WorkspacesBigDataPool_Spec) Initialize_From_WorkspacesBigDataPool_STATUS(source *WorkspacesBigDataPool_STATUS) error {

	// AutoPause
	if source.AutoPause != nil {
		var autoPause AutoPauseProperties
		err := autoPause.Initialize_From_AutoPauseProperties_STATUS(source.AutoPause)
		if err != nil {
			return errors.Wrap(err, "calling Initialize_From_AutoPauseProperties_STATUS() to populate field AutoPause")
		}
		pool.AutoPause = &autoPause
	} else {
		pool.AutoPause = nil
	}

	// AutoScale
	if source.AutoScale != nil {
		var autoScale AutoScaleProperties
		err := autoScale.Initialize_From_AutoScaleProperties_STATUS(source.AutoScale)
		if err != nil {
			return errors.Wrap(err, "calling Initialize_From_AutoScaleProperties_STATUS() to populate field AutoScale")
		}
		pool.AutoScale = &autoScale
	} else {
		pool.AutoScale = nil
	}

	// CacheSize
	pool.CacheSize = genruntime.ClonePointerToInt(source.CacheSize)

	// CustomLibraries
	if source.CustomLibraries != nil {
		customLibraryList := make([]LibraryInfo, len(source.CustomLibraries))
		for customLibraryIndex, customLibraryItem := range source.CustomLibraries {
			// Shadow the loop variable to avoid aliasing
			customLibraryItem := customLibraryItem
			var customLibrary LibraryInfo
			err := customLibrary.Initialize_From_LibraryInfo_STATUS(&customLibraryItem)
			if err != nil {
				return errors.Wrap(err, "calling Initialize_From_LibraryInfo_STATUS() to populate field CustomLibraries")
			}
			customLibraryList[customLibraryIndex] = customLibrary
		}
		pool.CustomLibraries = customLibraryList
	} else {
		pool.CustomLibraries = nil
	}

	// DefaultSparkLogFolder
	pool.DefaultSparkLogFolder = genruntime.ClonePointerToString(source.DefaultSparkLogFolder)

	// DynamicExecutorAllocation
	if source.DynamicExecutorAllocation != nil {
		var dynamicExecutorAllocation DynamicExecutorAllocation
		err := dynamicExecutorAllocation.Initialize_From_DynamicExecutorAllocation_STATUS(source.DynamicExecutorAllocation)
		if err != nil {
			return errors.Wrap(err, "calling Initialize_From_DynamicExecutorAllocation_STATUS() to populate field DynamicExecutorAllocation")
		}
		pool.DynamicExecutorAllocation = &dynamicExecutorAllocation
	} else {
		pool.DynamicExecutorAllocation = nil
	}

	// IsAutotuneEnabled
	if source.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *source.IsAutotuneEnabled
		pool.IsAutotuneEnabled = &isAutotuneEnabled
	} else {
		pool.IsAutotuneEnabled = nil
	}

	// IsComputeIsolationEnabled
	if source.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *source.IsComputeIsolationEnabled
		pool.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	} else {
		pool.IsComputeIsolationEnabled = nil
	}

	// LibraryRequirements
	if source.LibraryRequirements != nil {
		var libraryRequirement LibraryRequirements
		err := libraryRequirement.Initialize_From_LibraryRequirements_STATUS(source.LibraryRequirements)
		if err != nil {
			return errors.Wrap(err, "calling Initialize_From_LibraryRequirements_STATUS() to populate field LibraryRequirements")
		}
		pool.LibraryRequirements = &libraryRequirement
	} else {
		pool.LibraryRequirements = nil
	}

	// Location
	pool.Location = genruntime.ClonePointerToString(source.Location)

	// NodeCount
	pool.NodeCount = genruntime.ClonePointerToInt(source.NodeCount)

	// NodeSize
	if source.NodeSize != nil {
		nodeSize := genruntime.ToEnum(string(*source.NodeSize), bigDataPoolResourceProperties_NodeSize_Values)
		pool.NodeSize = &nodeSize
	} else {
		pool.NodeSize = nil
	}

	// NodeSizeFamily
	if source.NodeSizeFamily != nil {
		nodeSizeFamily := genruntime.ToEnum(string(*source.NodeSizeFamily), bigDataPoolResourceProperties_NodeSizeFamily_Values)
		pool.NodeSizeFamily = &nodeSizeFamily
	} else {
		pool.NodeSizeFamily = nil
	}

	// ProvisioningState
	pool.ProvisioningState = genruntime.ClonePointerToString(source.ProvisioningState)

	// SessionLevelPackagesEnabled
	if source.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *source.SessionLevelPackagesEnabled
		pool.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	} else {
		pool.SessionLevelPackagesEnabled = nil
	}

	// SparkConfigProperties
	if source.SparkConfigProperties != nil {
		var sparkConfigProperty SparkConfigProperties
		err := sparkConfigProperty.Initialize_From_SparkConfigProperties_STATUS(source.SparkConfigProperties)
		if err != nil {
			return errors.Wrap(err, "calling Initialize_From_SparkConfigProperties_STATUS() to populate field SparkConfigProperties")
		}
		pool.SparkConfigProperties = &sparkConfigProperty
	} else {
		pool.SparkConfigProperties = nil
	}

	// SparkEventsFolder
	pool.SparkEventsFolder = genruntime.ClonePointerToString(source.SparkEventsFolder)

	// SparkVersion
	pool.SparkVersion = genruntime.ClonePointerToString(source.SparkVersion)

	// Tags
	pool.Tags = genruntime.CloneMapOfStringToString(source.Tags)

	// No error
	return nil
}

// OriginalVersion returns the original API version used to create the resource.
func (pool *WorkspacesBigDataPool_Spec) OriginalVersion() string {
	return GroupVersion.Version
}

// SetAzureName sets the Azure name of the resource
func (pool *WorkspacesBigDataPool_Spec) SetAzureName(azureName string) { pool.AzureName = azureName }

type WorkspacesBigDataPool_STATUS struct {
	// AutoPause: Auto-pausing properties
	AutoPause *AutoPauseProperties_STATUS `json:"autoPause,omitempty"`

	// AutoScale: Auto-scaling properties
	AutoScale *AutoScaleProperties_STATUS `json:"autoScale,omitempty"`

	// CacheSize: The cache size
	CacheSize *int `json:"cacheSize,omitempty"`

	// Conditions: The observed state of the resource
	Conditions []conditions.Condition `json:"conditions,omitempty"`

	// CreationDate: The time when the Big Data pool was created.
	CreationDate *string `json:"creationDate,omitempty"`

	// CustomLibraries: List of custom libraries/packages associated with the spark pool.
	CustomLibraries []LibraryInfo_STATUS `json:"customLibraries,omitempty"`

	// DefaultSparkLogFolder: The default folder where Spark logs will be written.
	DefaultSparkLogFolder *string `json:"defaultSparkLogFolder,omitempty"`

	// DynamicExecutorAllocation: Dynamic Executor Allocation
	DynamicExecutorAllocation *DynamicExecutorAllocation_STATUS `json:"dynamicExecutorAllocation,omitempty"`

	// Id: Fully qualified resource ID for the resource. Ex -
	// /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}
	Id *string `json:"id,omitempty"`

	// IsAutotuneEnabled: Whether autotune is required or not.
	IsAutotuneEnabled *bool `json:"isAutotuneEnabled,omitempty"`

	// IsComputeIsolationEnabled: Whether compute isolation is required or not.
	IsComputeIsolationEnabled *bool `json:"isComputeIsolationEnabled,omitempty"`

	// LastSucceededTimestamp: The time when the Big Data pool was updated successfully.
	LastSucceededTimestamp *string `json:"lastSucceededTimestamp,omitempty"`

	// LibraryRequirements: Library version requirements
	LibraryRequirements *LibraryRequirements_STATUS `json:"libraryRequirements,omitempty"`

	// Location: The geo-location where the resource lives
	Location *string `json:"location,omitempty"`

	// Name: The name of the resource
	Name *string `json:"name,omitempty"`

	// NodeCount: The number of nodes in the Big Data pool.
	NodeCount *int `json:"nodeCount,omitempty"`

	// NodeSize: The level of compute power that each node in the Big Data pool has.
	NodeSize *BigDataPoolResourceProperties_NodeSize_STATUS `json:"nodeSize,omitempty"`

	// NodeSizeFamily: The kind of nodes that the Big Data pool provides.
	NodeSizeFamily *BigDataPoolResourceProperties_NodeSizeFamily_STATUS `json:"nodeSizeFamily,omitempty"`

	// ProvisioningState: The state of the Big Data pool.
	ProvisioningState *string `json:"provisioningState,omitempty"`

	// SessionLevelPackagesEnabled: Whether session level packages enabled.
	SessionLevelPackagesEnabled *bool `json:"sessionLevelPackagesEnabled,omitempty"`

	// SparkConfigProperties: Spark configuration file to specify additional properties
	SparkConfigProperties *SparkConfigProperties_STATUS `json:"sparkConfigProperties,omitempty"`

	// SparkEventsFolder: The Spark events folder
	SparkEventsFolder *string `json:"sparkEventsFolder,omitempty"`

	// SparkVersion: The Apache Spark version.
	SparkVersion *string `json:"sparkVersion,omitempty"`

	// Tags: Resource tags.
	Tags map[string]string `json:"tags,omitempty"`

	// Type: The type of the resource. E.g. "Microsoft.Compute/virtualMachines" or "Microsoft.Storage/storageAccounts"
	Type *string `json:"type,omitempty"`
}

var _ genruntime.ConvertibleStatus = &WorkspacesBigDataPool_STATUS{}

// ConvertStatusFrom populates our WorkspacesBigDataPool_STATUS from the provided source
func (pool *WorkspacesBigDataPool_STATUS) ConvertStatusFrom(source genruntime.ConvertibleStatus) error {
	src, ok := source.(*storage.WorkspacesBigDataPool_STATUS)
	if ok {
		// Populate our instance from source
		return pool.AssignProperties_From_WorkspacesBigDataPool_STATUS(src)
	}

	// Convert to an intermediate form
	src = &storage.WorkspacesBigDataPool_STATUS{}
	err := src.ConvertStatusFrom(source)
	if err != nil {
		return errors.Wrap(err, "initial step of conversion in ConvertStatusFrom()")
	}

	// Update our instance from src
	err = pool.AssignProperties_From_WorkspacesBigDataPool_STATUS(src)
	if err != nil {
		return errors.Wrap(err, "final step of conversion in ConvertStatusFrom()")
	}

	return nil
}

// ConvertStatusTo populates the provided destination from our WorkspacesBigDataPool_STATUS
func (pool *WorkspacesBigDataPool_STATUS) ConvertStatusTo(destination genruntime.ConvertibleStatus) error {
	dst, ok := destination.(*storage.WorkspacesBigDataPool_STATUS)
	if ok {
		// Populate destination from our instance
		return pool.AssignProperties_To_WorkspacesBigDataPool_STATUS(dst)
	}

	// Convert to an intermediate form
	dst = &storage.WorkspacesBigDataPool_STATUS{}
	err := pool.AssignProperties_To_WorkspacesBigDataPool_STATUS(dst)
	if err != nil {
		return errors.Wrap(err, "initial step of conversion in ConvertStatusTo()")
	}

	// Update dst from our instance
	err = dst.ConvertStatusTo(destination)
	if err != nil {
		return errors.Wrap(err, "final step of conversion in ConvertStatusTo()")
	}

	return nil
}

var _ genruntime.FromARMConverter = &WorkspacesBigDataPool_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (pool *WorkspacesBigDataPool_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.WorkspacesBigDataPool_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (pool *WorkspacesBigDataPool_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.WorkspacesBigDataPool_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.WorkspacesBigDataPool_STATUS, got %T", armInput)
	}

	// Set property "AutoPause":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.AutoPause != nil {
			var autoPause1 AutoPauseProperties_STATUS
			err := autoPause1.PopulateFromARM(owner, *typedInput.Properties.AutoPause)
			if err != nil {
				return err
			}
			autoPause := autoPause1
			pool.AutoPause = &autoPause
		}
	}

	// Set property "AutoScale":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.AutoScale != nil {
			var autoScale1 AutoScaleProperties_STATUS
			err := autoScale1.PopulateFromARM(owner, *typedInput.Properties.AutoScale)
			if err != nil {
				return err
			}
			autoScale := autoScale1
			pool.AutoScale = &autoScale
		}
	}

	// Set property "CacheSize":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.CacheSize != nil {
			cacheSize := *typedInput.Properties.CacheSize
			pool.CacheSize = &cacheSize
		}
	}

	// no assignment for property "Conditions"

	// Set property "CreationDate":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.CreationDate != nil {
			creationDate := *typedInput.Properties.CreationDate
			pool.CreationDate = &creationDate
		}
	}

	// Set property "CustomLibraries":
	// copying flattened property:
	if typedInput.Properties != nil {
		for _, item := range typedInput.Properties.CustomLibraries {
			var item1 LibraryInfo_STATUS
			err := item1.PopulateFromARM(owner, item)
			if err != nil {
				return err
			}
			pool.CustomLibraries = append(pool.CustomLibraries, item1)
		}
	}

	// Set property "DefaultSparkLogFolder":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.DefaultSparkLogFolder != nil {
			defaultSparkLogFolder := *typedInput.Properties.DefaultSparkLogFolder
			pool.DefaultSparkLogFolder = &defaultSparkLogFolder
		}
	}

	// Set property "DynamicExecutorAllocation":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.DynamicExecutorAllocation != nil {
			var dynamicExecutorAllocation1 DynamicExecutorAllocation_STATUS
			err := dynamicExecutorAllocation1.PopulateFromARM(owner, *typedInput.Properties.DynamicExecutorAllocation)
			if err != nil {
				return err
			}
			dynamicExecutorAllocation := dynamicExecutorAllocation1
			pool.DynamicExecutorAllocation = &dynamicExecutorAllocation
		}
	}

	// Set property "Id":
	if typedInput.Id != nil {
		id := *typedInput.Id
		pool.Id = &id
	}

	// Set property "IsAutotuneEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.IsAutotuneEnabled != nil {
			isAutotuneEnabled := *typedInput.Properties.IsAutotuneEnabled
			pool.IsAutotuneEnabled = &isAutotuneEnabled
		}
	}

	// Set property "IsComputeIsolationEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.IsComputeIsolationEnabled != nil {
			isComputeIsolationEnabled := *typedInput.Properties.IsComputeIsolationEnabled
			pool.IsComputeIsolationEnabled = &isComputeIsolationEnabled
		}
	}

	// Set property "LastSucceededTimestamp":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.LastSucceededTimestamp != nil {
			lastSucceededTimestamp := *typedInput.Properties.LastSucceededTimestamp
			pool.LastSucceededTimestamp = &lastSucceededTimestamp
		}
	}

	// Set property "LibraryRequirements":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.LibraryRequirements != nil {
			var libraryRequirements1 LibraryRequirements_STATUS
			err := libraryRequirements1.PopulateFromARM(owner, *typedInput.Properties.LibraryRequirements)
			if err != nil {
				return err
			}
			libraryRequirements := libraryRequirements1
			pool.LibraryRequirements = &libraryRequirements
		}
	}

	// Set property "Location":
	if typedInput.Location != nil {
		location := *typedInput.Location
		pool.Location = &location
	}

	// Set property "Name":
	if typedInput.Name != nil {
		name := *typedInput.Name
		pool.Name = &name
	}

	// Set property "NodeCount":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeCount != nil {
			nodeCount := *typedInput.Properties.NodeCount
			pool.NodeCount = &nodeCount
		}
	}

	// Set property "NodeSize":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeSize != nil {
			var temp string
			temp = string(*typedInput.Properties.NodeSize)
			nodeSize := BigDataPoolResourceProperties_NodeSize_STATUS(temp)
			pool.NodeSize = &nodeSize
		}
	}

	// Set property "NodeSizeFamily":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.NodeSizeFamily != nil {
			var temp string
			temp = string(*typedInput.Properties.NodeSizeFamily)
			nodeSizeFamily := BigDataPoolResourceProperties_NodeSizeFamily_STATUS(temp)
			pool.NodeSizeFamily = &nodeSizeFamily
		}
	}

	// Set property "ProvisioningState":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.ProvisioningState != nil {
			provisioningState := *typedInput.Properties.ProvisioningState
			pool.ProvisioningState = &provisioningState
		}
	}

	// Set property "SessionLevelPackagesEnabled":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SessionLevelPackagesEnabled != nil {
			sessionLevelPackagesEnabled := *typedInput.Properties.SessionLevelPackagesEnabled
			pool.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
		}
	}

	// Set property "SparkConfigProperties":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkConfigProperties != nil {
			var sparkConfigProperties1 SparkConfigProperties_STATUS
			err := sparkConfigProperties1.PopulateFromARM(owner, *typedInput.Properties.SparkConfigProperties)
			if err != nil {
				return err
			}
			sparkConfigProperties := sparkConfigProperties1
			pool.SparkConfigProperties = &sparkConfigProperties
		}
	}

	// Set property "SparkEventsFolder":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkEventsFolder != nil {
			sparkEventsFolder := *typedInput.Properties.SparkEventsFolder
			pool.SparkEventsFolder = &sparkEventsFolder
		}
	}

	// Set property "SparkVersion":
	// copying flattened property:
	if typedInput.Properties != nil {
		if typedInput.Properties.SparkVersion != nil {
			sparkVersion := *typedInput.Properties.SparkVersion
			pool.SparkVersion = &sparkVersion
		}
	}

	// Set property "Tags":
	if typedInput.Tags != nil {
		pool.Tags = make(map[string]string, len(typedInput.Tags))
		for key, value := range typedInput.Tags {
			pool.Tags[key] = value
		}
	}

	// Set property "Type":
	if typedInput.Type != nil {
		typeVar := *typedInput.Type
		pool.Type = &typeVar
	}

	// No error
	return nil
}

// AssignProperties_From_WorkspacesBigDataPool_STATUS populates our WorkspacesBigDataPool_STATUS from the provided source WorkspacesBigDataPool_STATUS
func (pool *WorkspacesBigDataPool_STATUS) AssignProperties_From_WorkspacesBigDataPool_STATUS(source *storage.WorkspacesBigDataPool_STATUS) error {

	// AutoPause
	if source.AutoPause != nil {
		var autoPause AutoPauseProperties_STATUS
		err := autoPause.AssignProperties_From_AutoPauseProperties_STATUS(source.AutoPause)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_AutoPauseProperties_STATUS() to populate field AutoPause")
		}
		pool.AutoPause = &autoPause
	} else {
		pool.AutoPause = nil
	}

	// AutoScale
	if source.AutoScale != nil {
		var autoScale AutoScaleProperties_STATUS
		err := autoScale.AssignProperties_From_AutoScaleProperties_STATUS(source.AutoScale)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_AutoScaleProperties_STATUS() to populate field AutoScale")
		}
		pool.AutoScale = &autoScale
	} else {
		pool.AutoScale = nil
	}

	// CacheSize
	pool.CacheSize = genruntime.ClonePointerToInt(source.CacheSize)

	// Conditions
	pool.Conditions = genruntime.CloneSliceOfCondition(source.Conditions)

	// CreationDate
	pool.CreationDate = genruntime.ClonePointerToString(source.CreationDate)

	// CustomLibraries
	if source.CustomLibraries != nil {
		customLibraryList := make([]LibraryInfo_STATUS, len(source.CustomLibraries))
		for customLibraryIndex, customLibraryItem := range source.CustomLibraries {
			// Shadow the loop variable to avoid aliasing
			customLibraryItem := customLibraryItem
			var customLibrary LibraryInfo_STATUS
			err := customLibrary.AssignProperties_From_LibraryInfo_STATUS(&customLibraryItem)
			if err != nil {
				return errors.Wrap(err, "calling AssignProperties_From_LibraryInfo_STATUS() to populate field CustomLibraries")
			}
			customLibraryList[customLibraryIndex] = customLibrary
		}
		pool.CustomLibraries = customLibraryList
	} else {
		pool.CustomLibraries = nil
	}

	// DefaultSparkLogFolder
	pool.DefaultSparkLogFolder = genruntime.ClonePointerToString(source.DefaultSparkLogFolder)

	// DynamicExecutorAllocation
	if source.DynamicExecutorAllocation != nil {
		var dynamicExecutorAllocation DynamicExecutorAllocation_STATUS
		err := dynamicExecutorAllocation.AssignProperties_From_DynamicExecutorAllocation_STATUS(source.DynamicExecutorAllocation)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_DynamicExecutorAllocation_STATUS() to populate field DynamicExecutorAllocation")
		}
		pool.DynamicExecutorAllocation = &dynamicExecutorAllocation
	} else {
		pool.DynamicExecutorAllocation = nil
	}

	// Id
	pool.Id = genruntime.ClonePointerToString(source.Id)

	// IsAutotuneEnabled
	if source.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *source.IsAutotuneEnabled
		pool.IsAutotuneEnabled = &isAutotuneEnabled
	} else {
		pool.IsAutotuneEnabled = nil
	}

	// IsComputeIsolationEnabled
	if source.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *source.IsComputeIsolationEnabled
		pool.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	} else {
		pool.IsComputeIsolationEnabled = nil
	}

	// LastSucceededTimestamp
	pool.LastSucceededTimestamp = genruntime.ClonePointerToString(source.LastSucceededTimestamp)

	// LibraryRequirements
	if source.LibraryRequirements != nil {
		var libraryRequirement LibraryRequirements_STATUS
		err := libraryRequirement.AssignProperties_From_LibraryRequirements_STATUS(source.LibraryRequirements)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_LibraryRequirements_STATUS() to populate field LibraryRequirements")
		}
		pool.LibraryRequirements = &libraryRequirement
	} else {
		pool.LibraryRequirements = nil
	}

	// Location
	pool.Location = genruntime.ClonePointerToString(source.Location)

	// Name
	pool.Name = genruntime.ClonePointerToString(source.Name)

	// NodeCount
	pool.NodeCount = genruntime.ClonePointerToInt(source.NodeCount)

	// NodeSize
	if source.NodeSize != nil {
		nodeSize := *source.NodeSize
		nodeSizeTemp := genruntime.ToEnum(nodeSize, bigDataPoolResourceProperties_NodeSize_STATUS_Values)
		pool.NodeSize = &nodeSizeTemp
	} else {
		pool.NodeSize = nil
	}

	// NodeSizeFamily
	if source.NodeSizeFamily != nil {
		nodeSizeFamily := *source.NodeSizeFamily
		nodeSizeFamilyTemp := genruntime.ToEnum(nodeSizeFamily, bigDataPoolResourceProperties_NodeSizeFamily_STATUS_Values)
		pool.NodeSizeFamily = &nodeSizeFamilyTemp
	} else {
		pool.NodeSizeFamily = nil
	}

	// ProvisioningState
	pool.ProvisioningState = genruntime.ClonePointerToString(source.ProvisioningState)

	// SessionLevelPackagesEnabled
	if source.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *source.SessionLevelPackagesEnabled
		pool.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	} else {
		pool.SessionLevelPackagesEnabled = nil
	}

	// SparkConfigProperties
	if source.SparkConfigProperties != nil {
		var sparkConfigProperty SparkConfigProperties_STATUS
		err := sparkConfigProperty.AssignProperties_From_SparkConfigProperties_STATUS(source.SparkConfigProperties)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_From_SparkConfigProperties_STATUS() to populate field SparkConfigProperties")
		}
		pool.SparkConfigProperties = &sparkConfigProperty
	} else {
		pool.SparkConfigProperties = nil
	}

	// SparkEventsFolder
	pool.SparkEventsFolder = genruntime.ClonePointerToString(source.SparkEventsFolder)

	// SparkVersion
	pool.SparkVersion = genruntime.ClonePointerToString(source.SparkVersion)

	// Tags
	pool.Tags = genruntime.CloneMapOfStringToString(source.Tags)

	// Type
	pool.Type = genruntime.ClonePointerToString(source.Type)

	// No error
	return nil
}

// AssignProperties_To_WorkspacesBigDataPool_STATUS populates the provided destination WorkspacesBigDataPool_STATUS from our WorkspacesBigDataPool_STATUS
func (pool *WorkspacesBigDataPool_STATUS) AssignProperties_To_WorkspacesBigDataPool_STATUS(destination *storage.WorkspacesBigDataPool_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// AutoPause
	if pool.AutoPause != nil {
		var autoPause storage.AutoPauseProperties_STATUS
		err := pool.AutoPause.AssignProperties_To_AutoPauseProperties_STATUS(&autoPause)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_AutoPauseProperties_STATUS() to populate field AutoPause")
		}
		destination.AutoPause = &autoPause
	} else {
		destination.AutoPause = nil
	}

	// AutoScale
	if pool.AutoScale != nil {
		var autoScale storage.AutoScaleProperties_STATUS
		err := pool.AutoScale.AssignProperties_To_AutoScaleProperties_STATUS(&autoScale)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_AutoScaleProperties_STATUS() to populate field AutoScale")
		}
		destination.AutoScale = &autoScale
	} else {
		destination.AutoScale = nil
	}

	// CacheSize
	destination.CacheSize = genruntime.ClonePointerToInt(pool.CacheSize)

	// Conditions
	destination.Conditions = genruntime.CloneSliceOfCondition(pool.Conditions)

	// CreationDate
	destination.CreationDate = genruntime.ClonePointerToString(pool.CreationDate)

	// CustomLibraries
	if pool.CustomLibraries != nil {
		customLibraryList := make([]storage.LibraryInfo_STATUS, len(pool.CustomLibraries))
		for customLibraryIndex, customLibraryItem := range pool.CustomLibraries {
			// Shadow the loop variable to avoid aliasing
			customLibraryItem := customLibraryItem
			var customLibrary storage.LibraryInfo_STATUS
			err := customLibraryItem.AssignProperties_To_LibraryInfo_STATUS(&customLibrary)
			if err != nil {
				return errors.Wrap(err, "calling AssignProperties_To_LibraryInfo_STATUS() to populate field CustomLibraries")
			}
			customLibraryList[customLibraryIndex] = customLibrary
		}
		destination.CustomLibraries = customLibraryList
	} else {
		destination.CustomLibraries = nil
	}

	// DefaultSparkLogFolder
	destination.DefaultSparkLogFolder = genruntime.ClonePointerToString(pool.DefaultSparkLogFolder)

	// DynamicExecutorAllocation
	if pool.DynamicExecutorAllocation != nil {
		var dynamicExecutorAllocation storage.DynamicExecutorAllocation_STATUS
		err := pool.DynamicExecutorAllocation.AssignProperties_To_DynamicExecutorAllocation_STATUS(&dynamicExecutorAllocation)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_DynamicExecutorAllocation_STATUS() to populate field DynamicExecutorAllocation")
		}
		destination.DynamicExecutorAllocation = &dynamicExecutorAllocation
	} else {
		destination.DynamicExecutorAllocation = nil
	}

	// Id
	destination.Id = genruntime.ClonePointerToString(pool.Id)

	// IsAutotuneEnabled
	if pool.IsAutotuneEnabled != nil {
		isAutotuneEnabled := *pool.IsAutotuneEnabled
		destination.IsAutotuneEnabled = &isAutotuneEnabled
	} else {
		destination.IsAutotuneEnabled = nil
	}

	// IsComputeIsolationEnabled
	if pool.IsComputeIsolationEnabled != nil {
		isComputeIsolationEnabled := *pool.IsComputeIsolationEnabled
		destination.IsComputeIsolationEnabled = &isComputeIsolationEnabled
	} else {
		destination.IsComputeIsolationEnabled = nil
	}

	// LastSucceededTimestamp
	destination.LastSucceededTimestamp = genruntime.ClonePointerToString(pool.LastSucceededTimestamp)

	// LibraryRequirements
	if pool.LibraryRequirements != nil {
		var libraryRequirement storage.LibraryRequirements_STATUS
		err := pool.LibraryRequirements.AssignProperties_To_LibraryRequirements_STATUS(&libraryRequirement)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_LibraryRequirements_STATUS() to populate field LibraryRequirements")
		}
		destination.LibraryRequirements = &libraryRequirement
	} else {
		destination.LibraryRequirements = nil
	}

	// Location
	destination.Location = genruntime.ClonePointerToString(pool.Location)

	// Name
	destination.Name = genruntime.ClonePointerToString(pool.Name)

	// NodeCount
	destination.NodeCount = genruntime.ClonePointerToInt(pool.NodeCount)

	// NodeSize
	if pool.NodeSize != nil {
		nodeSize := string(*pool.NodeSize)
		destination.NodeSize = &nodeSize
	} else {
		destination.NodeSize = nil
	}

	// NodeSizeFamily
	if pool.NodeSizeFamily != nil {
		nodeSizeFamily := string(*pool.NodeSizeFamily)
		destination.NodeSizeFamily = &nodeSizeFamily
	} else {
		destination.NodeSizeFamily = nil
	}

	// ProvisioningState
	destination.ProvisioningState = genruntime.ClonePointerToString(pool.ProvisioningState)

	// SessionLevelPackagesEnabled
	if pool.SessionLevelPackagesEnabled != nil {
		sessionLevelPackagesEnabled := *pool.SessionLevelPackagesEnabled
		destination.SessionLevelPackagesEnabled = &sessionLevelPackagesEnabled
	} else {
		destination.SessionLevelPackagesEnabled = nil
	}

	// SparkConfigProperties
	if pool.SparkConfigProperties != nil {
		var sparkConfigProperty storage.SparkConfigProperties_STATUS
		err := pool.SparkConfigProperties.AssignProperties_To_SparkConfigProperties_STATUS(&sparkConfigProperty)
		if err != nil {
			return errors.Wrap(err, "calling AssignProperties_To_SparkConfigProperties_STATUS() to populate field SparkConfigProperties")
		}
		destination.SparkConfigProperties = &sparkConfigProperty
	} else {
		destination.SparkConfigProperties = nil
	}

	// SparkEventsFolder
	destination.SparkEventsFolder = genruntime.ClonePointerToString(pool.SparkEventsFolder)

	// SparkVersion
	destination.SparkVersion = genruntime.ClonePointerToString(pool.SparkVersion)

	// Tags
	destination.Tags = genruntime.CloneMapOfStringToString(pool.Tags)

	// Type
	destination.Type = genruntime.ClonePointerToString(pool.Type)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Auto-pausing properties of a Big Data pool powered by Apache Spark
type AutoPauseProperties struct {
	// DelayInMinutes: Number of minutes of idle time before the Big Data pool is automatically paused.
	DelayInMinutes *int `json:"delayInMinutes,omitempty"`

	// Enabled: Whether auto-pausing is enabled for the Big Data pool.
	Enabled *bool `json:"enabled,omitempty"`
}

var _ genruntime.ARMTransformer = &AutoPauseProperties{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (properties *AutoPauseProperties) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if properties == nil {
		return nil, nil
	}
	result := &arm.AutoPauseProperties{}

	// Set property "DelayInMinutes":
	if properties.DelayInMinutes != nil {
		delayInMinutes := *properties.DelayInMinutes
		result.DelayInMinutes = &delayInMinutes
	}

	// Set property "Enabled":
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		result.Enabled = &enabled
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *AutoPauseProperties) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.AutoPauseProperties{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *AutoPauseProperties) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.AutoPauseProperties)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.AutoPauseProperties, got %T", armInput)
	}

	// Set property "DelayInMinutes":
	if typedInput.DelayInMinutes != nil {
		delayInMinutes := *typedInput.DelayInMinutes
		properties.DelayInMinutes = &delayInMinutes
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		properties.Enabled = &enabled
	}

	// No error
	return nil
}

// AssignProperties_From_AutoPauseProperties populates our AutoPauseProperties from the provided source AutoPauseProperties
func (properties *AutoPauseProperties) AssignProperties_From_AutoPauseProperties(source *storage.AutoPauseProperties) error {

	// DelayInMinutes
	properties.DelayInMinutes = genruntime.ClonePointerToInt(source.DelayInMinutes)

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// No error
	return nil
}

// AssignProperties_To_AutoPauseProperties populates the provided destination AutoPauseProperties from our AutoPauseProperties
func (properties *AutoPauseProperties) AssignProperties_To_AutoPauseProperties(destination *storage.AutoPauseProperties) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// DelayInMinutes
	destination.DelayInMinutes = genruntime.ClonePointerToInt(properties.DelayInMinutes)

	// Enabled
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_AutoPauseProperties_STATUS populates our AutoPauseProperties from the provided source AutoPauseProperties_STATUS
func (properties *AutoPauseProperties) Initialize_From_AutoPauseProperties_STATUS(source *AutoPauseProperties_STATUS) error {

	// DelayInMinutes
	properties.DelayInMinutes = genruntime.ClonePointerToInt(source.DelayInMinutes)

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// No error
	return nil
}

// Auto-pausing properties of a Big Data pool powered by Apache Spark
type AutoPauseProperties_STATUS struct {
	// DelayInMinutes: Number of minutes of idle time before the Big Data pool is automatically paused.
	DelayInMinutes *int `json:"delayInMinutes,omitempty"`

	// Enabled: Whether auto-pausing is enabled for the Big Data pool.
	Enabled *bool `json:"enabled,omitempty"`
}

var _ genruntime.FromARMConverter = &AutoPauseProperties_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *AutoPauseProperties_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.AutoPauseProperties_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *AutoPauseProperties_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.AutoPauseProperties_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.AutoPauseProperties_STATUS, got %T", armInput)
	}

	// Set property "DelayInMinutes":
	if typedInput.DelayInMinutes != nil {
		delayInMinutes := *typedInput.DelayInMinutes
		properties.DelayInMinutes = &delayInMinutes
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		properties.Enabled = &enabled
	}

	// No error
	return nil
}

// AssignProperties_From_AutoPauseProperties_STATUS populates our AutoPauseProperties_STATUS from the provided source AutoPauseProperties_STATUS
func (properties *AutoPauseProperties_STATUS) AssignProperties_From_AutoPauseProperties_STATUS(source *storage.AutoPauseProperties_STATUS) error {

	// DelayInMinutes
	properties.DelayInMinutes = genruntime.ClonePointerToInt(source.DelayInMinutes)

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// No error
	return nil
}

// AssignProperties_To_AutoPauseProperties_STATUS populates the provided destination AutoPauseProperties_STATUS from our AutoPauseProperties_STATUS
func (properties *AutoPauseProperties_STATUS) AssignProperties_To_AutoPauseProperties_STATUS(destination *storage.AutoPauseProperties_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// DelayInMinutes
	destination.DelayInMinutes = genruntime.ClonePointerToInt(properties.DelayInMinutes)

	// Enabled
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Auto-scaling properties of a Big Data pool powered by Apache Spark
type AutoScaleProperties struct {
	// Enabled: Whether automatic scaling is enabled for the Big Data pool.
	Enabled *bool `json:"enabled,omitempty"`

	// MaxNodeCount: The maximum number of nodes the Big Data pool can support.
	MaxNodeCount *int `json:"maxNodeCount,omitempty"`

	// MinNodeCount: The minimum number of nodes the Big Data pool can support.
	MinNodeCount *int `json:"minNodeCount,omitempty"`
}

var _ genruntime.ARMTransformer = &AutoScaleProperties{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (properties *AutoScaleProperties) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if properties == nil {
		return nil, nil
	}
	result := &arm.AutoScaleProperties{}

	// Set property "Enabled":
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		result.Enabled = &enabled
	}

	// Set property "MaxNodeCount":
	if properties.MaxNodeCount != nil {
		maxNodeCount := *properties.MaxNodeCount
		result.MaxNodeCount = &maxNodeCount
	}

	// Set property "MinNodeCount":
	if properties.MinNodeCount != nil {
		minNodeCount := *properties.MinNodeCount
		result.MinNodeCount = &minNodeCount
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *AutoScaleProperties) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.AutoScaleProperties{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *AutoScaleProperties) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.AutoScaleProperties)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.AutoScaleProperties, got %T", armInput)
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		properties.Enabled = &enabled
	}

	// Set property "MaxNodeCount":
	if typedInput.MaxNodeCount != nil {
		maxNodeCount := *typedInput.MaxNodeCount
		properties.MaxNodeCount = &maxNodeCount
	}

	// Set property "MinNodeCount":
	if typedInput.MinNodeCount != nil {
		minNodeCount := *typedInput.MinNodeCount
		properties.MinNodeCount = &minNodeCount
	}

	// No error
	return nil
}

// AssignProperties_From_AutoScaleProperties populates our AutoScaleProperties from the provided source AutoScaleProperties
func (properties *AutoScaleProperties) AssignProperties_From_AutoScaleProperties(source *storage.AutoScaleProperties) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// MaxNodeCount
	properties.MaxNodeCount = genruntime.ClonePointerToInt(source.MaxNodeCount)

	// MinNodeCount
	properties.MinNodeCount = genruntime.ClonePointerToInt(source.MinNodeCount)

	// No error
	return nil
}

// AssignProperties_To_AutoScaleProperties populates the provided destination AutoScaleProperties from our AutoScaleProperties
func (properties *AutoScaleProperties) AssignProperties_To_AutoScaleProperties(destination *storage.AutoScaleProperties) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Enabled
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// MaxNodeCount
	destination.MaxNodeCount = genruntime.ClonePointerToInt(properties.MaxNodeCount)

	// MinNodeCount
	destination.MinNodeCount = genruntime.ClonePointerToInt(properties.MinNodeCount)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_AutoScaleProperties_STATUS populates our AutoScaleProperties from the provided source AutoScaleProperties_STATUS
func (properties *AutoScaleProperties) Initialize_From_AutoScaleProperties_STATUS(source *AutoScaleProperties_STATUS) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// MaxNodeCount
	properties.MaxNodeCount = genruntime.ClonePointerToInt(source.MaxNodeCount)

	// MinNodeCount
	properties.MinNodeCount = genruntime.ClonePointerToInt(source.MinNodeCount)

	// No error
	return nil
}

// Auto-scaling properties of a Big Data pool powered by Apache Spark
type AutoScaleProperties_STATUS struct {
	// Enabled: Whether automatic scaling is enabled for the Big Data pool.
	Enabled *bool `json:"enabled,omitempty"`

	// MaxNodeCount: The maximum number of nodes the Big Data pool can support.
	MaxNodeCount *int `json:"maxNodeCount,omitempty"`

	// MinNodeCount: The minimum number of nodes the Big Data pool can support.
	MinNodeCount *int `json:"minNodeCount,omitempty"`
}

var _ genruntime.FromARMConverter = &AutoScaleProperties_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *AutoScaleProperties_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.AutoScaleProperties_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *AutoScaleProperties_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.AutoScaleProperties_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.AutoScaleProperties_STATUS, got %T", armInput)
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		properties.Enabled = &enabled
	}

	// Set property "MaxNodeCount":
	if typedInput.MaxNodeCount != nil {
		maxNodeCount := *typedInput.MaxNodeCount
		properties.MaxNodeCount = &maxNodeCount
	}

	// Set property "MinNodeCount":
	if typedInput.MinNodeCount != nil {
		minNodeCount := *typedInput.MinNodeCount
		properties.MinNodeCount = &minNodeCount
	}

	// No error
	return nil
}

// AssignProperties_From_AutoScaleProperties_STATUS populates our AutoScaleProperties_STATUS from the provided source AutoScaleProperties_STATUS
func (properties *AutoScaleProperties_STATUS) AssignProperties_From_AutoScaleProperties_STATUS(source *storage.AutoScaleProperties_STATUS) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		properties.Enabled = &enabled
	} else {
		properties.Enabled = nil
	}

	// MaxNodeCount
	properties.MaxNodeCount = genruntime.ClonePointerToInt(source.MaxNodeCount)

	// MinNodeCount
	properties.MinNodeCount = genruntime.ClonePointerToInt(source.MinNodeCount)

	// No error
	return nil
}

// AssignProperties_To_AutoScaleProperties_STATUS populates the provided destination AutoScaleProperties_STATUS from our AutoScaleProperties_STATUS
func (properties *AutoScaleProperties_STATUS) AssignProperties_To_AutoScaleProperties_STATUS(destination *storage.AutoScaleProperties_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Enabled
	if properties.Enabled != nil {
		enabled := *properties.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// MaxNodeCount
	destination.MaxNodeCount = genruntime.ClonePointerToInt(properties.MaxNodeCount)

	// MinNodeCount
	destination.MinNodeCount = genruntime.ClonePointerToInt(properties.MinNodeCount)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// +kubebuilder:validation:Enum={"Large","Medium","None","Small","XLarge","XXLarge","XXXLarge"}
type BigDataPoolResourceProperties_NodeSize string

const (
	BigDataPoolResourceProperties_NodeSize_Large    = BigDataPoolResourceProperties_NodeSize("Large")
	BigDataPoolResourceProperties_NodeSize_Medium   = BigDataPoolResourceProperties_NodeSize("Medium")
	BigDataPoolResourceProperties_NodeSize_None     = BigDataPoolResourceProperties_NodeSize("None")
	BigDataPoolResourceProperties_NodeSize_Small    = BigDataPoolResourceProperties_NodeSize("Small")
	BigDataPoolResourceProperties_NodeSize_XLarge   = BigDataPoolResourceProperties_NodeSize("XLarge")
	BigDataPoolResourceProperties_NodeSize_XXLarge  = BigDataPoolResourceProperties_NodeSize("XXLarge")
	BigDataPoolResourceProperties_NodeSize_XXXLarge = BigDataPoolResourceProperties_NodeSize("XXXLarge")
)

// Mapping from string to BigDataPoolResourceProperties_NodeSize
var bigDataPoolResourceProperties_NodeSize_Values = map[string]BigDataPoolResourceProperties_NodeSize{
	"large":    BigDataPoolResourceProperties_NodeSize_Large,
	"medium":   BigDataPoolResourceProperties_NodeSize_Medium,
	"none":     BigDataPoolResourceProperties_NodeSize_None,
	"small":    BigDataPoolResourceProperties_NodeSize_Small,
	"xlarge":   BigDataPoolResourceProperties_NodeSize_XLarge,
	"xxlarge":  BigDataPoolResourceProperties_NodeSize_XXLarge,
	"xxxlarge": BigDataPoolResourceProperties_NodeSize_XXXLarge,
}

type BigDataPoolResourceProperties_NodeSize_STATUS string

const (
	BigDataPoolResourceProperties_NodeSize_STATUS_Large    = BigDataPoolResourceProperties_NodeSize_STATUS("Large")
	BigDataPoolResourceProperties_NodeSize_STATUS_Medium   = BigDataPoolResourceProperties_NodeSize_STATUS("Medium")
	BigDataPoolResourceProperties_NodeSize_STATUS_None     = BigDataPoolResourceProperties_NodeSize_STATUS("None")
	BigDataPoolResourceProperties_NodeSize_STATUS_Small    = BigDataPoolResourceProperties_NodeSize_STATUS("Small")
	BigDataPoolResourceProperties_NodeSize_STATUS_XLarge   = BigDataPoolResourceProperties_NodeSize_STATUS("XLarge")
	BigDataPoolResourceProperties_NodeSize_STATUS_XXLarge  = BigDataPoolResourceProperties_NodeSize_STATUS("XXLarge")
	BigDataPoolResourceProperties_NodeSize_STATUS_XXXLarge = BigDataPoolResourceProperties_NodeSize_STATUS("XXXLarge")
)

// Mapping from string to BigDataPoolResourceProperties_NodeSize_STATUS
var bigDataPoolResourceProperties_NodeSize_STATUS_Values = map[string]BigDataPoolResourceProperties_NodeSize_STATUS{
	"large":    BigDataPoolResourceProperties_NodeSize_STATUS_Large,
	"medium":   BigDataPoolResourceProperties_NodeSize_STATUS_Medium,
	"none":     BigDataPoolResourceProperties_NodeSize_STATUS_None,
	"small":    BigDataPoolResourceProperties_NodeSize_STATUS_Small,
	"xlarge":   BigDataPoolResourceProperties_NodeSize_STATUS_XLarge,
	"xxlarge":  BigDataPoolResourceProperties_NodeSize_STATUS_XXLarge,
	"xxxlarge": BigDataPoolResourceProperties_NodeSize_STATUS_XXXLarge,
}

// +kubebuilder:validation:Enum={"HardwareAcceleratedFPGA","HardwareAcceleratedGPU","MemoryOptimized","None"}
type BigDataPoolResourceProperties_NodeSizeFamily string

const (
	BigDataPoolResourceProperties_NodeSizeFamily_HardwareAcceleratedFPGA = BigDataPoolResourceProperties_NodeSizeFamily("HardwareAcceleratedFPGA")
	BigDataPoolResourceProperties_NodeSizeFamily_HardwareAcceleratedGPU  = BigDataPoolResourceProperties_NodeSizeFamily("HardwareAcceleratedGPU")
	BigDataPoolResourceProperties_NodeSizeFamily_MemoryOptimized         = BigDataPoolResourceProperties_NodeSizeFamily("MemoryOptimized")
	BigDataPoolResourceProperties_NodeSizeFamily_None                    = BigDataPoolResourceProperties_NodeSizeFamily("None")
)

// Mapping from string to BigDataPoolResourceProperties_NodeSizeFamily
var bigDataPoolResourceProperties_NodeSizeFamily_Values = map[string]BigDataPoolResourceProperties_NodeSizeFamily{
	"hardwareacceleratedfpga": BigDataPoolResourceProperties_NodeSizeFamily_HardwareAcceleratedFPGA,
	"hardwareacceleratedgpu":  BigDataPoolResourceProperties_NodeSizeFamily_HardwareAcceleratedGPU,
	"memoryoptimized":         BigDataPoolResourceProperties_NodeSizeFamily_MemoryOptimized,
	"none":                    BigDataPoolResourceProperties_NodeSizeFamily_None,
}

type BigDataPoolResourceProperties_NodeSizeFamily_STATUS string

const (
	BigDataPoolResourceProperties_NodeSizeFamily_STATUS_HardwareAcceleratedFPGA = BigDataPoolResourceProperties_NodeSizeFamily_STATUS("HardwareAcceleratedFPGA")
	BigDataPoolResourceProperties_NodeSizeFamily_STATUS_HardwareAcceleratedGPU  = BigDataPoolResourceProperties_NodeSizeFamily_STATUS("HardwareAcceleratedGPU")
	BigDataPoolResourceProperties_NodeSizeFamily_STATUS_MemoryOptimized         = BigDataPoolResourceProperties_NodeSizeFamily_STATUS("MemoryOptimized")
	BigDataPoolResourceProperties_NodeSizeFamily_STATUS_None                    = BigDataPoolResourceProperties_NodeSizeFamily_STATUS("None")
)

// Mapping from string to BigDataPoolResourceProperties_NodeSizeFamily_STATUS
var bigDataPoolResourceProperties_NodeSizeFamily_STATUS_Values = map[string]BigDataPoolResourceProperties_NodeSizeFamily_STATUS{
	"hardwareacceleratedfpga": BigDataPoolResourceProperties_NodeSizeFamily_STATUS_HardwareAcceleratedFPGA,
	"hardwareacceleratedgpu":  BigDataPoolResourceProperties_NodeSizeFamily_STATUS_HardwareAcceleratedGPU,
	"memoryoptimized":         BigDataPoolResourceProperties_NodeSizeFamily_STATUS_MemoryOptimized,
	"none":                    BigDataPoolResourceProperties_NodeSizeFamily_STATUS_None,
}

// Dynamic Executor Allocation Properties
type DynamicExecutorAllocation struct {
	// Enabled: Indicates whether Dynamic Executor Allocation is enabled or not.
	Enabled *bool `json:"enabled,omitempty"`

	// MaxExecutors: The maximum number of executors alloted
	MaxExecutors *int `json:"maxExecutors,omitempty"`

	// MinExecutors: The minimum number of executors alloted
	MinExecutors *int `json:"minExecutors,omitempty"`
}

var _ genruntime.ARMTransformer = &DynamicExecutorAllocation{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (allocation *DynamicExecutorAllocation) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if allocation == nil {
		return nil, nil
	}
	result := &arm.DynamicExecutorAllocation{}

	// Set property "Enabled":
	if allocation.Enabled != nil {
		enabled := *allocation.Enabled
		result.Enabled = &enabled
	}

	// Set property "MaxExecutors":
	if allocation.MaxExecutors != nil {
		maxExecutors := *allocation.MaxExecutors
		result.MaxExecutors = &maxExecutors
	}

	// Set property "MinExecutors":
	if allocation.MinExecutors != nil {
		minExecutors := *allocation.MinExecutors
		result.MinExecutors = &minExecutors
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (allocation *DynamicExecutorAllocation) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.DynamicExecutorAllocation{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (allocation *DynamicExecutorAllocation) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.DynamicExecutorAllocation)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.DynamicExecutorAllocation, got %T", armInput)
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		allocation.Enabled = &enabled
	}

	// Set property "MaxExecutors":
	if typedInput.MaxExecutors != nil {
		maxExecutors := *typedInput.MaxExecutors
		allocation.MaxExecutors = &maxExecutors
	}

	// Set property "MinExecutors":
	if typedInput.MinExecutors != nil {
		minExecutors := *typedInput.MinExecutors
		allocation.MinExecutors = &minExecutors
	}

	// No error
	return nil
}

// AssignProperties_From_DynamicExecutorAllocation populates our DynamicExecutorAllocation from the provided source DynamicExecutorAllocation
func (allocation *DynamicExecutorAllocation) AssignProperties_From_DynamicExecutorAllocation(source *storage.DynamicExecutorAllocation) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		allocation.Enabled = &enabled
	} else {
		allocation.Enabled = nil
	}

	// MaxExecutors
	allocation.MaxExecutors = genruntime.ClonePointerToInt(source.MaxExecutors)

	// MinExecutors
	allocation.MinExecutors = genruntime.ClonePointerToInt(source.MinExecutors)

	// No error
	return nil
}

// AssignProperties_To_DynamicExecutorAllocation populates the provided destination DynamicExecutorAllocation from our DynamicExecutorAllocation
func (allocation *DynamicExecutorAllocation) AssignProperties_To_DynamicExecutorAllocation(destination *storage.DynamicExecutorAllocation) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Enabled
	if allocation.Enabled != nil {
		enabled := *allocation.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// MaxExecutors
	destination.MaxExecutors = genruntime.ClonePointerToInt(allocation.MaxExecutors)

	// MinExecutors
	destination.MinExecutors = genruntime.ClonePointerToInt(allocation.MinExecutors)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_DynamicExecutorAllocation_STATUS populates our DynamicExecutorAllocation from the provided source DynamicExecutorAllocation_STATUS
func (allocation *DynamicExecutorAllocation) Initialize_From_DynamicExecutorAllocation_STATUS(source *DynamicExecutorAllocation_STATUS) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		allocation.Enabled = &enabled
	} else {
		allocation.Enabled = nil
	}

	// MaxExecutors
	allocation.MaxExecutors = genruntime.ClonePointerToInt(source.MaxExecutors)

	// MinExecutors
	allocation.MinExecutors = genruntime.ClonePointerToInt(source.MinExecutors)

	// No error
	return nil
}

// Dynamic Executor Allocation Properties
type DynamicExecutorAllocation_STATUS struct {
	// Enabled: Indicates whether Dynamic Executor Allocation is enabled or not.
	Enabled *bool `json:"enabled,omitempty"`

	// MaxExecutors: The maximum number of executors alloted
	MaxExecutors *int `json:"maxExecutors,omitempty"`

	// MinExecutors: The minimum number of executors alloted
	MinExecutors *int `json:"minExecutors,omitempty"`
}

var _ genruntime.FromARMConverter = &DynamicExecutorAllocation_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (allocation *DynamicExecutorAllocation_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.DynamicExecutorAllocation_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (allocation *DynamicExecutorAllocation_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.DynamicExecutorAllocation_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.DynamicExecutorAllocation_STATUS, got %T", armInput)
	}

	// Set property "Enabled":
	if typedInput.Enabled != nil {
		enabled := *typedInput.Enabled
		allocation.Enabled = &enabled
	}

	// Set property "MaxExecutors":
	if typedInput.MaxExecutors != nil {
		maxExecutors := *typedInput.MaxExecutors
		allocation.MaxExecutors = &maxExecutors
	}

	// Set property "MinExecutors":
	if typedInput.MinExecutors != nil {
		minExecutors := *typedInput.MinExecutors
		allocation.MinExecutors = &minExecutors
	}

	// No error
	return nil
}

// AssignProperties_From_DynamicExecutorAllocation_STATUS populates our DynamicExecutorAllocation_STATUS from the provided source DynamicExecutorAllocation_STATUS
func (allocation *DynamicExecutorAllocation_STATUS) AssignProperties_From_DynamicExecutorAllocation_STATUS(source *storage.DynamicExecutorAllocation_STATUS) error {

	// Enabled
	if source.Enabled != nil {
		enabled := *source.Enabled
		allocation.Enabled = &enabled
	} else {
		allocation.Enabled = nil
	}

	// MaxExecutors
	allocation.MaxExecutors = genruntime.ClonePointerToInt(source.MaxExecutors)

	// MinExecutors
	allocation.MinExecutors = genruntime.ClonePointerToInt(source.MinExecutors)

	// No error
	return nil
}

// AssignProperties_To_DynamicExecutorAllocation_STATUS populates the provided destination DynamicExecutorAllocation_STATUS from our DynamicExecutorAllocation_STATUS
func (allocation *DynamicExecutorAllocation_STATUS) AssignProperties_To_DynamicExecutorAllocation_STATUS(destination *storage.DynamicExecutorAllocation_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Enabled
	if allocation.Enabled != nil {
		enabled := *allocation.Enabled
		destination.Enabled = &enabled
	} else {
		destination.Enabled = nil
	}

	// MaxExecutors
	destination.MaxExecutors = genruntime.ClonePointerToInt(allocation.MaxExecutors)

	// MinExecutors
	destination.MinExecutors = genruntime.ClonePointerToInt(allocation.MinExecutors)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Library/package information of a Big Data pool powered by Apache Spark
type LibraryInfo struct {
	// ContainerName: Storage blob container name.
	ContainerName *string `json:"containerName,omitempty"`

	// Name: Name of the library.
	Name *string `json:"name,omitempty"`

	// Path: Storage blob path of library.
	Path *string `json:"path,omitempty"`

	// Type: Type of the library.
	Type *string `json:"type,omitempty"`
}

var _ genruntime.ARMTransformer = &LibraryInfo{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (info *LibraryInfo) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if info == nil {
		return nil, nil
	}
	result := &arm.LibraryInfo{}

	// Set property "ContainerName":
	if info.ContainerName != nil {
		containerName := *info.ContainerName
		result.ContainerName = &containerName
	}

	// Set property "Name":
	if info.Name != nil {
		name := *info.Name
		result.Name = &name
	}

	// Set property "Path":
	if info.Path != nil {
		path := *info.Path
		result.Path = &path
	}

	// Set property "Type":
	if info.Type != nil {
		typeVar := *info.Type
		result.Type = &typeVar
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (info *LibraryInfo) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.LibraryInfo{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (info *LibraryInfo) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.LibraryInfo)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.LibraryInfo, got %T", armInput)
	}

	// Set property "ContainerName":
	if typedInput.ContainerName != nil {
		containerName := *typedInput.ContainerName
		info.ContainerName = &containerName
	}

	// Set property "Name":
	if typedInput.Name != nil {
		name := *typedInput.Name
		info.Name = &name
	}

	// Set property "Path":
	if typedInput.Path != nil {
		path := *typedInput.Path
		info.Path = &path
	}

	// Set property "Type":
	if typedInput.Type != nil {
		typeVar := *typedInput.Type
		info.Type = &typeVar
	}

	// No error
	return nil
}

// AssignProperties_From_LibraryInfo populates our LibraryInfo from the provided source LibraryInfo
func (info *LibraryInfo) AssignProperties_From_LibraryInfo(source *storage.LibraryInfo) error {

	// ContainerName
	info.ContainerName = genruntime.ClonePointerToString(source.ContainerName)

	// Name
	info.Name = genruntime.ClonePointerToString(source.Name)

	// Path
	info.Path = genruntime.ClonePointerToString(source.Path)

	// Type
	info.Type = genruntime.ClonePointerToString(source.Type)

	// No error
	return nil
}

// AssignProperties_To_LibraryInfo populates the provided destination LibraryInfo from our LibraryInfo
func (info *LibraryInfo) AssignProperties_To_LibraryInfo(destination *storage.LibraryInfo) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// ContainerName
	destination.ContainerName = genruntime.ClonePointerToString(info.ContainerName)

	// Name
	destination.Name = genruntime.ClonePointerToString(info.Name)

	// Path
	destination.Path = genruntime.ClonePointerToString(info.Path)

	// Type
	destination.Type = genruntime.ClonePointerToString(info.Type)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_LibraryInfo_STATUS populates our LibraryInfo from the provided source LibraryInfo_STATUS
func (info *LibraryInfo) Initialize_From_LibraryInfo_STATUS(source *LibraryInfo_STATUS) error {

	// ContainerName
	info.ContainerName = genruntime.ClonePointerToString(source.ContainerName)

	// Name
	info.Name = genruntime.ClonePointerToString(source.Name)

	// Path
	info.Path = genruntime.ClonePointerToString(source.Path)

	// Type
	info.Type = genruntime.ClonePointerToString(source.Type)

	// No error
	return nil
}

// Library/package information of a Big Data pool powered by Apache Spark
type LibraryInfo_STATUS struct {
	// ContainerName: Storage blob container name.
	ContainerName *string `json:"containerName,omitempty"`

	// CreatorId: Creator Id of the library/package.
	CreatorId *string `json:"creatorId,omitempty"`

	// Name: Name of the library.
	Name *string `json:"name,omitempty"`

	// Path: Storage blob path of library.
	Path *string `json:"path,omitempty"`

	// ProvisioningStatus: Provisioning status of the library/package.
	ProvisioningStatus *string `json:"provisioningStatus,omitempty"`

	// Type: Type of the library.
	Type *string `json:"type,omitempty"`

	// UploadedTimestamp: The last update time of the library.
	UploadedTimestamp *string `json:"uploadedTimestamp,omitempty"`
}

var _ genruntime.FromARMConverter = &LibraryInfo_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (info *LibraryInfo_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.LibraryInfo_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (info *LibraryInfo_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.LibraryInfo_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.LibraryInfo_STATUS, got %T", armInput)
	}

	// Set property "ContainerName":
	if typedInput.ContainerName != nil {
		containerName := *typedInput.ContainerName
		info.ContainerName = &containerName
	}

	// Set property "CreatorId":
	if typedInput.CreatorId != nil {
		creatorId := *typedInput.CreatorId
		info.CreatorId = &creatorId
	}

	// Set property "Name":
	if typedInput.Name != nil {
		name := *typedInput.Name
		info.Name = &name
	}

	// Set property "Path":
	if typedInput.Path != nil {
		path := *typedInput.Path
		info.Path = &path
	}

	// Set property "ProvisioningStatus":
	if typedInput.ProvisioningStatus != nil {
		provisioningStatus := *typedInput.ProvisioningStatus
		info.ProvisioningStatus = &provisioningStatus
	}

	// Set property "Type":
	if typedInput.Type != nil {
		typeVar := *typedInput.Type
		info.Type = &typeVar
	}

	// Set property "UploadedTimestamp":
	if typedInput.UploadedTimestamp != nil {
		uploadedTimestamp := *typedInput.UploadedTimestamp
		info.UploadedTimestamp = &uploadedTimestamp
	}

	// No error
	return nil
}

// AssignProperties_From_LibraryInfo_STATUS populates our LibraryInfo_STATUS from the provided source LibraryInfo_STATUS
func (info *LibraryInfo_STATUS) AssignProperties_From_LibraryInfo_STATUS(source *storage.LibraryInfo_STATUS) error {

	// ContainerName
	info.ContainerName = genruntime.ClonePointerToString(source.ContainerName)

	// CreatorId
	info.CreatorId = genruntime.ClonePointerToString(source.CreatorId)

	// Name
	info.Name = genruntime.ClonePointerToString(source.Name)

	// Path
	info.Path = genruntime.ClonePointerToString(source.Path)

	// ProvisioningStatus
	info.ProvisioningStatus = genruntime.ClonePointerToString(source.ProvisioningStatus)

	// Type
	info.Type = genruntime.ClonePointerToString(source.Type)

	// UploadedTimestamp
	info.UploadedTimestamp = genruntime.ClonePointerToString(source.UploadedTimestamp)

	// No error
	return nil
}

// AssignProperties_To_LibraryInfo_STATUS populates the provided destination LibraryInfo_STATUS from our LibraryInfo_STATUS
func (info *LibraryInfo_STATUS) AssignProperties_To_LibraryInfo_STATUS(destination *storage.LibraryInfo_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// ContainerName
	destination.ContainerName = genruntime.ClonePointerToString(info.ContainerName)

	// CreatorId
	destination.CreatorId = genruntime.ClonePointerToString(info.CreatorId)

	// Name
	destination.Name = genruntime.ClonePointerToString(info.Name)

	// Path
	destination.Path = genruntime.ClonePointerToString(info.Path)

	// ProvisioningStatus
	destination.ProvisioningStatus = genruntime.ClonePointerToString(info.ProvisioningStatus)

	// Type
	destination.Type = genruntime.ClonePointerToString(info.Type)

	// UploadedTimestamp
	destination.UploadedTimestamp = genruntime.ClonePointerToString(info.UploadedTimestamp)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Library requirements for a Big Data pool powered by Apache Spark
type LibraryRequirements struct {
	// Content: The library requirements.
	Content *string `json:"content,omitempty"`

	// Filename: The filename of the library requirements file.
	Filename *string `json:"filename,omitempty"`
}

var _ genruntime.ARMTransformer = &LibraryRequirements{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (requirements *LibraryRequirements) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if requirements == nil {
		return nil, nil
	}
	result := &arm.LibraryRequirements{}

	// Set property "Content":
	if requirements.Content != nil {
		content := *requirements.Content
		result.Content = &content
	}

	// Set property "Filename":
	if requirements.Filename != nil {
		filename := *requirements.Filename
		result.Filename = &filename
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (requirements *LibraryRequirements) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.LibraryRequirements{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (requirements *LibraryRequirements) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.LibraryRequirements)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.LibraryRequirements, got %T", armInput)
	}

	// Set property "Content":
	if typedInput.Content != nil {
		content := *typedInput.Content
		requirements.Content = &content
	}

	// Set property "Filename":
	if typedInput.Filename != nil {
		filename := *typedInput.Filename
		requirements.Filename = &filename
	}

	// No error
	return nil
}

// AssignProperties_From_LibraryRequirements populates our LibraryRequirements from the provided source LibraryRequirements
func (requirements *LibraryRequirements) AssignProperties_From_LibraryRequirements(source *storage.LibraryRequirements) error {

	// Content
	requirements.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	requirements.Filename = genruntime.ClonePointerToString(source.Filename)

	// No error
	return nil
}

// AssignProperties_To_LibraryRequirements populates the provided destination LibraryRequirements from our LibraryRequirements
func (requirements *LibraryRequirements) AssignProperties_To_LibraryRequirements(destination *storage.LibraryRequirements) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Content
	destination.Content = genruntime.ClonePointerToString(requirements.Content)

	// Filename
	destination.Filename = genruntime.ClonePointerToString(requirements.Filename)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_LibraryRequirements_STATUS populates our LibraryRequirements from the provided source LibraryRequirements_STATUS
func (requirements *LibraryRequirements) Initialize_From_LibraryRequirements_STATUS(source *LibraryRequirements_STATUS) error {

	// Content
	requirements.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	requirements.Filename = genruntime.ClonePointerToString(source.Filename)

	// No error
	return nil
}

// Library requirements for a Big Data pool powered by Apache Spark
type LibraryRequirements_STATUS struct {
	// Content: The library requirements.
	Content *string `json:"content,omitempty"`

	// Filename: The filename of the library requirements file.
	Filename *string `json:"filename,omitempty"`

	// Time: The last update time of the library requirements file.
	Time *string `json:"time,omitempty"`
}

var _ genruntime.FromARMConverter = &LibraryRequirements_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (requirements *LibraryRequirements_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.LibraryRequirements_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (requirements *LibraryRequirements_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.LibraryRequirements_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.LibraryRequirements_STATUS, got %T", armInput)
	}

	// Set property "Content":
	if typedInput.Content != nil {
		content := *typedInput.Content
		requirements.Content = &content
	}

	// Set property "Filename":
	if typedInput.Filename != nil {
		filename := *typedInput.Filename
		requirements.Filename = &filename
	}

	// Set property "Time":
	if typedInput.Time != nil {
		time := *typedInput.Time
		requirements.Time = &time
	}

	// No error
	return nil
}

// AssignProperties_From_LibraryRequirements_STATUS populates our LibraryRequirements_STATUS from the provided source LibraryRequirements_STATUS
func (requirements *LibraryRequirements_STATUS) AssignProperties_From_LibraryRequirements_STATUS(source *storage.LibraryRequirements_STATUS) error {

	// Content
	requirements.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	requirements.Filename = genruntime.ClonePointerToString(source.Filename)

	// Time
	requirements.Time = genruntime.ClonePointerToString(source.Time)

	// No error
	return nil
}

// AssignProperties_To_LibraryRequirements_STATUS populates the provided destination LibraryRequirements_STATUS from our LibraryRequirements_STATUS
func (requirements *LibraryRequirements_STATUS) AssignProperties_To_LibraryRequirements_STATUS(destination *storage.LibraryRequirements_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// Content
	destination.Content = genruntime.ClonePointerToString(requirements.Content)

	// Filename
	destination.Filename = genruntime.ClonePointerToString(requirements.Filename)

	// Time
	destination.Time = genruntime.ClonePointerToString(requirements.Time)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// SparkConfig Properties for a Big Data pool powered by Apache Spark
type SparkConfigProperties struct {
	// ConfigurationType: The type of the spark config properties file.
	ConfigurationType *SparkConfigProperties_ConfigurationType `json:"configurationType,omitempty"`

	// Content: The spark config properties.
	Content *string `json:"content,omitempty"`

	// Filename: The filename of the spark config properties file.
	Filename *string `json:"filename,omitempty"`
}

var _ genruntime.ARMTransformer = &SparkConfigProperties{}

// ConvertToARM converts from a Kubernetes CRD object to an ARM object
func (properties *SparkConfigProperties) ConvertToARM(resolved genruntime.ConvertToARMResolvedDetails) (interface{}, error) {
	if properties == nil {
		return nil, nil
	}
	result := &arm.SparkConfigProperties{}

	// Set property "ConfigurationType":
	if properties.ConfigurationType != nil {
		var temp string
		temp = string(*properties.ConfigurationType)
		configurationType := arm.SparkConfigProperties_ConfigurationType(temp)
		result.ConfigurationType = &configurationType
	}

	// Set property "Content":
	if properties.Content != nil {
		content := *properties.Content
		result.Content = &content
	}

	// Set property "Filename":
	if properties.Filename != nil {
		filename := *properties.Filename
		result.Filename = &filename
	}
	return result, nil
}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *SparkConfigProperties) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.SparkConfigProperties{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *SparkConfigProperties) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.SparkConfigProperties)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.SparkConfigProperties, got %T", armInput)
	}

	// Set property "ConfigurationType":
	if typedInput.ConfigurationType != nil {
		var temp string
		temp = string(*typedInput.ConfigurationType)
		configurationType := SparkConfigProperties_ConfigurationType(temp)
		properties.ConfigurationType = &configurationType
	}

	// Set property "Content":
	if typedInput.Content != nil {
		content := *typedInput.Content
		properties.Content = &content
	}

	// Set property "Filename":
	if typedInput.Filename != nil {
		filename := *typedInput.Filename
		properties.Filename = &filename
	}

	// No error
	return nil
}

// AssignProperties_From_SparkConfigProperties populates our SparkConfigProperties from the provided source SparkConfigProperties
func (properties *SparkConfigProperties) AssignProperties_From_SparkConfigProperties(source *storage.SparkConfigProperties) error {

	// ConfigurationType
	if source.ConfigurationType != nil {
		configurationType := *source.ConfigurationType
		configurationTypeTemp := genruntime.ToEnum(configurationType, sparkConfigProperties_ConfigurationType_Values)
		properties.ConfigurationType = &configurationTypeTemp
	} else {
		properties.ConfigurationType = nil
	}

	// Content
	properties.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	properties.Filename = genruntime.ClonePointerToString(source.Filename)

	// No error
	return nil
}

// AssignProperties_To_SparkConfigProperties populates the provided destination SparkConfigProperties from our SparkConfigProperties
func (properties *SparkConfigProperties) AssignProperties_To_SparkConfigProperties(destination *storage.SparkConfigProperties) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// ConfigurationType
	if properties.ConfigurationType != nil {
		configurationType := string(*properties.ConfigurationType)
		destination.ConfigurationType = &configurationType
	} else {
		destination.ConfigurationType = nil
	}

	// Content
	destination.Content = genruntime.ClonePointerToString(properties.Content)

	// Filename
	destination.Filename = genruntime.ClonePointerToString(properties.Filename)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Initialize_From_SparkConfigProperties_STATUS populates our SparkConfigProperties from the provided source SparkConfigProperties_STATUS
func (properties *SparkConfigProperties) Initialize_From_SparkConfigProperties_STATUS(source *SparkConfigProperties_STATUS) error {

	// ConfigurationType
	if source.ConfigurationType != nil {
		configurationType := genruntime.ToEnum(string(*source.ConfigurationType), sparkConfigProperties_ConfigurationType_Values)
		properties.ConfigurationType = &configurationType
	} else {
		properties.ConfigurationType = nil
	}

	// Content
	properties.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	properties.Filename = genruntime.ClonePointerToString(source.Filename)

	// No error
	return nil
}

// SparkConfig Properties for a Big Data pool powered by Apache Spark
type SparkConfigProperties_STATUS struct {
	// ConfigurationType: The type of the spark config properties file.
	ConfigurationType *SparkConfigProperties_ConfigurationType_STATUS `json:"configurationType,omitempty"`

	// Content: The spark config properties.
	Content *string `json:"content,omitempty"`

	// Filename: The filename of the spark config properties file.
	Filename *string `json:"filename,omitempty"`

	// Time: The last update time of the spark config properties file.
	Time *string `json:"time,omitempty"`
}

var _ genruntime.FromARMConverter = &SparkConfigProperties_STATUS{}

// NewEmptyARMValue returns an empty ARM value suitable for deserializing into
func (properties *SparkConfigProperties_STATUS) NewEmptyARMValue() genruntime.ARMResourceStatus {
	return &arm.SparkConfigProperties_STATUS{}
}

// PopulateFromARM populates a Kubernetes CRD object from an Azure ARM object
func (properties *SparkConfigProperties_STATUS) PopulateFromARM(owner genruntime.ArbitraryOwnerReference, armInput interface{}) error {
	typedInput, ok := armInput.(arm.SparkConfigProperties_STATUS)
	if !ok {
		return fmt.Errorf("unexpected type supplied for PopulateFromARM() function. Expected arm.SparkConfigProperties_STATUS, got %T", armInput)
	}

	// Set property "ConfigurationType":
	if typedInput.ConfigurationType != nil {
		var temp string
		temp = string(*typedInput.ConfigurationType)
		configurationType := SparkConfigProperties_ConfigurationType_STATUS(temp)
		properties.ConfigurationType = &configurationType
	}

	// Set property "Content":
	if typedInput.Content != nil {
		content := *typedInput.Content
		properties.Content = &content
	}

	// Set property "Filename":
	if typedInput.Filename != nil {
		filename := *typedInput.Filename
		properties.Filename = &filename
	}

	// Set property "Time":
	if typedInput.Time != nil {
		time := *typedInput.Time
		properties.Time = &time
	}

	// No error
	return nil
}

// AssignProperties_From_SparkConfigProperties_STATUS populates our SparkConfigProperties_STATUS from the provided source SparkConfigProperties_STATUS
func (properties *SparkConfigProperties_STATUS) AssignProperties_From_SparkConfigProperties_STATUS(source *storage.SparkConfigProperties_STATUS) error {

	// ConfigurationType
	if source.ConfigurationType != nil {
		configurationType := *source.ConfigurationType
		configurationTypeTemp := genruntime.ToEnum(configurationType, sparkConfigProperties_ConfigurationType_STATUS_Values)
		properties.ConfigurationType = &configurationTypeTemp
	} else {
		properties.ConfigurationType = nil
	}

	// Content
	properties.Content = genruntime.ClonePointerToString(source.Content)

	// Filename
	properties.Filename = genruntime.ClonePointerToString(source.Filename)

	// Time
	properties.Time = genruntime.ClonePointerToString(source.Time)

	// No error
	return nil
}

// AssignProperties_To_SparkConfigProperties_STATUS populates the provided destination SparkConfigProperties_STATUS from our SparkConfigProperties_STATUS
func (properties *SparkConfigProperties_STATUS) AssignProperties_To_SparkConfigProperties_STATUS(destination *storage.SparkConfigProperties_STATUS) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// ConfigurationType
	if properties.ConfigurationType != nil {
		configurationType := string(*properties.ConfigurationType)
		destination.ConfigurationType = &configurationType
	} else {
		destination.ConfigurationType = nil
	}

	// Content
	destination.Content = genruntime.ClonePointerToString(properties.Content)

	// Filename
	destination.Filename = genruntime.ClonePointerToString(properties.Filename)

	// Time
	destination.Time = genruntime.ClonePointerToString(properties.Time)

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// Details for configuring operator behavior. Fields in this struct are interpreted by the operator directly rather than being passed to Azure
type WorkspacesBigDataPoolOperatorSpec struct {
	// ConfigMapExpressions: configures where to place operator written dynamic ConfigMaps (created with CEL expressions).
	ConfigMapExpressions []*core.DestinationExpression `json:"configMapExpressions,omitempty"`

	// SecretExpressions: configures where to place operator written dynamic secrets (created with CEL expressions).
	SecretExpressions []*core.DestinationExpression `json:"secretExpressions,omitempty"`
}

// AssignProperties_From_WorkspacesBigDataPoolOperatorSpec populates our WorkspacesBigDataPoolOperatorSpec from the provided source WorkspacesBigDataPoolOperatorSpec
func (operator *WorkspacesBigDataPoolOperatorSpec) AssignProperties_From_WorkspacesBigDataPoolOperatorSpec(source *storage.WorkspacesBigDataPoolOperatorSpec) error {

	// ConfigMapExpressions
	if source.ConfigMapExpressions != nil {
		configMapExpressionList := make([]*core.DestinationExpression, len(source.ConfigMapExpressions))
		for configMapExpressionIndex, configMapExpressionItem := range source.ConfigMapExpressions {
			// Shadow the loop variable to avoid aliasing
			configMapExpressionItem := configMapExpressionItem
			if configMapExpressionItem != nil {
				configMapExpression := *configMapExpressionItem.DeepCopy()
				configMapExpressionList[configMapExpressionIndex] = &configMapExpression
			} else {
				configMapExpressionList[configMapExpressionIndex] = nil
			}
		}
		operator.ConfigMapExpressions = configMapExpressionList
	} else {
		operator.ConfigMapExpressions = nil
	}

	// SecretExpressions
	if source.SecretExpressions != nil {
		secretExpressionList := make([]*core.DestinationExpression, len(source.SecretExpressions))
		for secretExpressionIndex, secretExpressionItem := range source.SecretExpressions {
			// Shadow the loop variable to avoid aliasing
			secretExpressionItem := secretExpressionItem
			if secretExpressionItem != nil {
				secretExpression := *secretExpressionItem.DeepCopy()
				secretExpressionList[secretExpressionIndex] = &secretExpression
			} else {
				secretExpressionList[secretExpressionIndex] = nil
			}
		}
		operator.SecretExpressions = secretExpressionList
	} else {
		operator.SecretExpressions = nil
	}

	// No error
	return nil
}

// AssignProperties_To_WorkspacesBigDataPoolOperatorSpec populates the provided destination WorkspacesBigDataPoolOperatorSpec from our WorkspacesBigDataPoolOperatorSpec
func (operator *WorkspacesBigDataPoolOperatorSpec) AssignProperties_To_WorkspacesBigDataPoolOperatorSpec(destination *storage.WorkspacesBigDataPoolOperatorSpec) error {
	// Create a new property bag
	propertyBag := genruntime.NewPropertyBag()

	// ConfigMapExpressions
	if operator.ConfigMapExpressions != nil {
		configMapExpressionList := make([]*core.DestinationExpression, len(operator.ConfigMapExpressions))
		for configMapExpressionIndex, configMapExpressionItem := range operator.ConfigMapExpressions {
			// Shadow the loop variable to avoid aliasing
			configMapExpressionItem := configMapExpressionItem
			if configMapExpressionItem != nil {
				configMapExpression := *configMapExpressionItem.DeepCopy()
				configMapExpressionList[configMapExpressionIndex] = &configMapExpression
			} else {
				configMapExpressionList[configMapExpressionIndex] = nil
			}
		}
		destination.ConfigMapExpressions = configMapExpressionList
	} else {
		destination.ConfigMapExpressions = nil
	}

	// SecretExpressions
	if operator.SecretExpressions != nil {
		secretExpressionList := make([]*core.DestinationExpression, len(operator.SecretExpressions))
		for secretExpressionIndex, secretExpressionItem := range operator.SecretExpressions {
			// Shadow the loop variable to avoid aliasing
			secretExpressionItem := secretExpressionItem
			if secretExpressionItem != nil {
				secretExpression := *secretExpressionItem.DeepCopy()
				secretExpressionList[secretExpressionIndex] = &secretExpression
			} else {
				secretExpressionList[secretExpressionIndex] = nil
			}
		}
		destination.SecretExpressions = secretExpressionList
	} else {
		destination.SecretExpressions = nil
	}

	// Update the property bag
	if len(propertyBag) > 0 {
		destination.PropertyBag = propertyBag
	} else {
		destination.PropertyBag = nil
	}

	// No error
	return nil
}

// +kubebuilder:validation:Enum={"Artifact","File"}
type SparkConfigProperties_ConfigurationType string

const (
	SparkConfigProperties_ConfigurationType_Artifact = SparkConfigProperties_ConfigurationType("Artifact")
	SparkConfigProperties_ConfigurationType_File     = SparkConfigProperties_ConfigurationType("File")
)

// Mapping from string to SparkConfigProperties_ConfigurationType
var sparkConfigProperties_ConfigurationType_Values = map[string]SparkConfigProperties_ConfigurationType{
	"artifact": SparkConfigProperties_ConfigurationType_Artifact,
	"file":     SparkConfigProperties_ConfigurationType_File,
}

type SparkConfigProperties_ConfigurationType_STATUS string

const (
	SparkConfigProperties_ConfigurationType_STATUS_Artifact = SparkConfigProperties_ConfigurationType_STATUS("Artifact")
	SparkConfigProperties_ConfigurationType_STATUS_File     = SparkConfigProperties_ConfigurationType_STATUS("File")
)

// Mapping from string to SparkConfigProperties_ConfigurationType_STATUS
var sparkConfigProperties_ConfigurationType_STATUS_Values = map[string]SparkConfigProperties_ConfigurationType_STATUS{
	"artifact": SparkConfigProperties_ConfigurationType_STATUS_Artifact,
	"file":     SparkConfigProperties_ConfigurationType_STATUS_File,
}

func init() {
	SchemeBuilder.Register(&WorkspacesBigDataPool{}, &WorkspacesBigDataPoolList{})
}
